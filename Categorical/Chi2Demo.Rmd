---
title: "Chi Squared"
output: 
  pdf_document: 
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Instructions
----------
  The data comes from the faculty salary example.

  There are three variables: 
    sex (sex of professor)
      1 = male
      2 = female
    rank (rank of professor)
      1 = full professor
      2 = associate professor
      3 = assistant professor
      4 = instructor
    level (type of program that professor teaches in)
      1 = doctoral program
      2 = masters program  
        
```{r}
library(pacman) #Package used to load all packages using p_load(); will install missing packages
p_load(vcd, MASS, jmv, gmodels)
# jmv and gmodels used for chi-squared
# vcd, MASS used for loglinear
```
Load your data
```{r}
dat <- read.csv("https://www.dropbox.com/s/w2bcd0c2n7qgwzz/Salary-1.csv?dl=1")
head(dat)
```

While this part isn't necessary it will make this entire demo easier to read.
You are relabeling the levels of each variable. 
```{r}
dat$sex <- factor(dat$sex, levels = c(1,2), labels = c("Male", "Female"))
dat$rank <- factor(dat$rank, levels = c(1,2,3,4), labels = c("Full", "Associate", "Assistant", "Instructor"))
dat$level <- factor(dat$level, levels = c(1,2), labels = c("Doctorate", "Masters"))
head(dat)
```

## Goodness of Fit
Observed Frequencies for each variable.
```{r}
sex <- table(dat$sex)
sex

rank <- table(dat$rank)
rank

level <- table(dat$level)
level

## uses descriptives from jmv library - it is mas cute

desc <- descriptives(data = dat, 
                     vars = c('sex', 'rank', 'level'), 
                     freq = TRUE)
desc

```

Assumptions
  - 1. Adequate expected cell counts
    - 5 or more in 2 x 2 or 5 or more in 80% of cells for larger table
    - Otherwise, Fisher's test
  - 2. Independence of Observations
    - otherwise McNemar's test of dependent proportions

Chi Squared Test Goodness of fit (testing if all frequencies are equal)
```{r}
# H0 = equal proportions in each category; Ha = unequal proportions in each category
# Chi-square = Sum[(Observed - Expected)^2/Expected]
# df = # of categories - 1
jmv::propTestN(data = dat,
               var = 'sex',
               expected = TRUE, 
               ratio = c(1,1))

jmv::propTestN(data = dat,
               var = 'rank',
               expected = TRUE, 
               ratio = c(1,1,1,1))

jmv::propTestN(data = dat,
               var = 'level',
               expected = TRUE, 
               ratio = c(1,1))

```

# However, what if we expected the proportions to be a little different. For example, based on an educated guess:  
  44% full Professors,  
  28% Associate Professors,  
  26% Assistant Professors,  
  2% Instructors

# How does it compare to the Chi-square where all levels were expected to have equal proportions?
```{r}
# H0 = baseline model proportions; Ha = significantly different than baseline model proportions
jmv::propTestN(data = dat,
               var = 'rank',
               expected = TRUE, 
               ratio = c(.44, .28, .26, .02))
```

## Chi-square Test of Independence
Ha: Is sex dependent upon rank? Is there a relationship between sex and rank? 

We have a *new* effect size here (Cramer's V), what does it mean in the context of these results?
```{r}
# Chi-square = Sum[(Observed - Expected)^2/Expected]
# Expected = [(# of row entries for cel)/(# total entries)] * (# of column entries for cel)
# Expected indicates expected values for each category if there is no relationship between two categorical variables
# df = (# rows - 1) * (# columns - 1)
# Cramer's V - small = .1; medium = .3, large = .5; discrepancy between observed and expected scores
jmv::contTables(dat = dat,
                rows = 'sex',
                cols = 'rank',
                exp = TRUE,
                phiCra = TRUE)
# report APA, magnitude of effect (Cramer's V), direction of effect example (more or less than expected in each category - see Contingency Table)
```

### Chi-square Test of Independence
Is level dependent upon sex? Is there a relationship between level and sex?
```{r}
jmv::contTables(dat = dat,
                rows = 'sex',
                cols = 'level',
                exp = TRUE,
                phiCra = TRUE)
```

### Chi-square Test of Independence
How about for rank and level?
```{r}
jmv::contTables(dat = dat,
                rows = 'rank',
                cols = 'level',
                exp = TRUE,
                phiCra = TRUE)
```

# What happens if we take this a step further...

What if our research question asks: is there a three-way contingency (sex x rank x level)?
df1 = # cells for sex - 1 = 2 - 1 = 1
df2 = # cells for rank - 1 = 4 - 1 = 3
df3 = # cells for level - 1 = 2 -1 = 1
N = number of cells in table (2 x 4 x 2) - df1 - df2 - df3
df = N - 1 = 10

Three way contingency test require *log-linear modeling*.  
Start with the independence model and end with the saturated model. 

**Evidence for model fit: non-significant chi-square value**
- no discrepancy between observed and expected values under the null model

Model 1, There are no relationships among the variables.

```{r}
# overall model test
# 2 x 4 x 2 contingency table
# Observed = mytable
# Expected = loglm
# Expected = Expected frequencies in 2 x 4 x 2 table if there are no relationships

# Null hypothesis means that expected frequencies satisfy our model of expected values
# Alternative Hypothesis means that difference between expected and observed frequencies is significant (indicates our model does not fit)

mytable<- xtabs(~dat$sex + dat$rank + dat$level) # table of observed values
model1 <- loglm(~dat$sex + dat$rank + dat$level, mytable)
mytable
summary(model1)


```

Model 2: Rank and Sex are *independent* but Rank/Level are related and Sex/Level are *related*.
```{r}
model2 <- loglm(~(dat$rank+dat$sex)*dat$level, mytable)
summary(model2)
```

Model 3: *All two-way* relationships
```{r}
model3 <- loglm(~dat$rank*dat$level + dat$level*dat$sex + dat$rank*dat$sex, mytable)
summary(model3)
```

Model 4: All two-way relationships *and the three-way* relationship
```{r}
#saturated model or "overfit model
# this takes us one step past parsimony
# this means that the three-way relationship does not add to the model

# i.e. Chi-squared is zero
# e.g., no degrees of freedom
model4 <- loglm(~dat$rank*dat$level*dat$sex, mytable)
summary(model4)
```

Compare Models
```{r}
stats::anova(model1,model2,model3, model4)

#Delta(Dev) is a chi-squared difference test between models
#once difference is no longer significant, the first model is likely parsimonious fit
```

The JMV way produces a cleaner output, but there are some drawbacks. Overall, it's good to know multiple ways but see which may be best for your analyses or purpose(s). For now, stick with loglm function.
```{r}
# note the similarities between 'Deviance' values and the model comparison stats with the loglm output. 
# the top table output is unknown - so look it up 

jmv::logLinear(
  data = dat,
  counts = NULL,
  factors = c('sex', 'rank', 'level'),
  blocks = list(
    list(
      'sex', 'rank', 'level'),
    list(
      c('sex', 'level'),
      c('rank', 'level')),
    list(
      c('sex', 'rank')),
    list(
      c('sex', 'rank', 'level'))),
  refLevels = list(
    list(
      var = 'sex',
      ref = 'Male'),
    list(
      var = 'rank',
      ref = 'Full'),
    list(
      var = 'level',
      ref = 'Doctorate')),
  modelTest = TRUE)



```