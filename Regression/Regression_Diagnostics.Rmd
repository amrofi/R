---
title: "Regression_Diagnostics"
author: "Wonderful TA"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Multiple Regression Diagnostics Demo**

Prompt:

A local resident recently joined their church's newly developed charitable giving program and is interested in finding out more about prosocial behavior in members of the congregation. Without knowing much about the literature, they develop a short survey to give to a nearby school district to find out what influences the amount of money given to their program for community outreach. Ultimately, they must report their findings to the program coordinator and a statistician on the board.

Variables:

Belief - belief that charitable giving has a positive effect on a scale of 1-10.
Need - rating of perceived amount of need required in the community on a scale of 1-10.
Interest - rating of level of interest in the project on a scale of 1-10.
Happy - rating of happiness felt when making donations on a scale of 1-10.
Amount - amount given from 0 - 10 dollars.

For all variables 99 = missing.


```{r}
dat <- read.csv("https://www.dropbox.com/s/gpo25w5vxkkpm0b/RegDiagnostics2.csv?dl=1")
library(psych)
library(jmv)
library(car)

#install these packages first
library(Hmisc)
library(MVN)
library(mice)
library(VIM)


dat[dat=="99"] <- NA

```


**Descriptive Statistics**
```{r}
# Descriptives
desc_OG <- descriptives(data = dat, 
                        vars = c('Belief', 'Need', 'Interest', 'Happy', 'Amount'), 
                        sd = TRUE, 
                        skew = TRUE, 
                        kurt = TRUE)
desc_OG

corr.test(dat)

#First thing to note is the MISSING DATA --> Different N's and the line that literally indicates how many are missing shows there are missing cases
#Options: (1) delete list-wise (2) impute 
```


**MISSING DATA**
```{r}
#check the pattern of missing data
md.pattern(dat)

mice_plot <-aggr(dat, 
                 col=c('navyblue', 'yellow'), 
                 numbers = TRUE, 
                 sortVars = TRUE, 
                 labels = names(dat), 
                 cex.axis = .7, 
                 gap = 3, 
                 ylab = c("Missing data", "Pattern"))

#yellow bar chart is percentage missing from each variable
#blue and yellow chart shows pattern of missing data --> no pattern here
```


```{r}
# Option 1: Listwise deletion of missing data. New dataset is named "dat.no.NA"
dat.no.NA <- na.omit(dat)

#check descriptives again
#N is all 100 now and no missing data
desc_listwise <- descriptives(data = dat.no.NA, 
                              vars = c('Belief', 'Need', 'Interest', 'Happy', 'Amount'))
desc_listwise

```


```{r}
#OR Option 2: Impute with scores

#Median
dat$Belief_imp <- round(with(dat, impute(Belief)), 2)
dat$Need_imp <- round(with(dat, impute(Need)), 2)
dat$Interest_imp <- round(with(dat, impute(Interest)), 2)
dat$Happy_imp <- round(with(dat, impute(Happy)), 2)
dat$Amount_imp <- round(with(dat, impute(Amount)), 2)


#check descriptives again
#N is all 106 now and no missing data
desc_imputed <- descriptives(data = dat, 
                             vars = c('Belief_imp', 'Need_imp', 'Interest_imp', 'Happy_imp', 'Amount_imp'))
desc_imputed


```


**UNIVARIATE NORMALITY AND OUTLIERS**
```{r}
#Big data set, can drop a few cases --> so going to continue on with more conservative "delete list-wise" data set

#check skew, kurtosis, and histograms for each variable (especially DV)
desc2 <- descriptives(data = dat.no.NA, 
                      vars = c('Belief', 'Need', 'Interest', 'Happy', 'Amount'), 
                      hist = TRUE, 
                      sd = TRUE, 
                      range = TRUE, 
                      skew = TRUE, 
                      kurt = TRUE)
desc2

#Amount (which is the DV) --> skew ~ 2 // kurtosis > 10

```


```{r}
#Identify outliers
#Scale() converts to z scores
dat.no.NA[abs(scale(dat.no.NA$Belief)) > 3, ]
dat.no.NA[abs(scale(dat.no.NA$Need)) > 3, ]
dat.no.NA[abs(scale(dat.no.NA$Interest)) > 3, ]
dat.no.NA[abs(scale(dat.no.NA$Happy)) > 3, ]
dat.no.NA[abs(scale(dat.no.NA$Amount)) > 3, ]

#Belief has 2 univariate outliers
#Happy has 1 univariate outlier
#Amount has 2 univariate outliers

```


```{r}
#Remove outliers
#Order to remove matters - look up for loop for this ugly code
dat.no.uni1 <- dat.no.NA[!abs(scale(dat.no.NA$Belief)) > 3, ]
dat.no.uni2 <- dat.no.uni1[!abs(scale(dat.no.uni1$Need)) > 3, ]
dat.no.uni3 <- dat.no.uni2[!abs(scale(dat.no.uni2$Interest)) > 3, ]
dat.no.uni4 <- dat.no.uni3[!abs(scale(dat.no.uni3$Happy)) > 3, ]
dat.no.uni5 <- dat.no.uni4[!abs(scale(dat.no.uni4$Amount)) > 3, ]

dat.no.uni <- dat.no.uni5

#Removed 5 cases that were outside +/-3 SD's for the variables

#Check descriptives to see how many people now and what the descriptives look like now
desc_no.uni <- descriptives(data = dat.no.uni, 
                            vars = c('Belief', 'Need', 'Interest', 'Happy', 'Amount'), 
                            hist = TRUE, 
                            sd = TRUE, 
                            range = TRUE, 
                            skew = TRUE, 
                            kurt = TRUE)
desc_no.uni

#data set is now called "dat.no.uni5"
#everything is now within range of normal distribution

```


**TRANSFORMATIONS**
```{r}
#Removing the outliers fixed _this_ normality issue, but had it not fixed it you can always transform data to fix normality issues

#Square root transformation for moderate skew
#Negative skew --> reflect and square root
dat.no.NA$Amount_T <- sqrt(max(dat.no.NA$Amount) + 1 - dat.no.NA$Amount)

# If positive skew then dat.no.NA$Amount_T <- sqrt(dat.no.NA$Amount)

#Log transformation for hella skew
#Negative skew --> reflect and log
dat.no.NA$Amount_T2 <- log10(max(dat.no.NA$Amount) + 1 - dat.no.NA$Amount)

head(dat.no.NA)

#check out descriptives for skew and kurtosis before and after all transforms
desc_transformed <- descriptives(data = dat.no.NA, 
                                 vars = c('Amount', 'Amount_T', 'Amount_T2'), 
                                 hist = TRUE, 
                                 sd = TRUE, 
                                 range = TRUE, 
                                 skew = TRUE, 
                                 kurt = TRUE)
desc_transformed

```


**MULTIVARIATE NORMALITY AND OUTLIERS**
```{r}

#look at your residuals and the Q-Q plot
model <- linReg(data = dat.no.uni, 
                dep = 'Amount', 
                covs = c('Belief', 'Need', 'Interest', 'Happy'),
                blocks = list(c('Belief', 'Need', 'Interest', 'Happy')), 
                modelTest = TRUE, 
                r2Adj = TRUE, 
                stdEst = TRUE, 
                ciStdEst = TRUE,
                qqPlot = TRUE,
                resPlots = TRUE) 
model

#Alternate
#model <- lm(Amount ~ Belief + Need, data = dat.no.uni)
#plot(model)

```


```{r}
#Check and remove multivariate outliers based on mahalanobis' distance
#MD = Leverage = how much each observation contributes to the model's prediction

#Create a variable of mahalanobis' distance
#This calculation only works with continuous variables
x <- dat.no.uni[1:5] #predictor variables subset columns
mean <- colMeans(x) #mean of each column
Sx <- cov(x)        #covariance of predictors
dat.no.uni$mahal <- mahalanobis(x, mean, Sx)

#Identify any multivariate outliers based on mahalanobis' distance variable --> +/- 3 SD's of mahalanobis distance mean
dat.no.uni[abs(scale(dat.no.uni$mahal)) > 3, ]

#There are 2 multivariate outliers

#Remove multivariate outliers
dat.no.mult <- dat.no.uni[!abs(scale(dat.no.uni$mahal)) > 3,]

```


```{r}
#OR Check and remove multivariate outliers based on cook's distance
#CD = Influence = Leverage + Discrepancy (Discrepancy = how much an observation deviates from the overall pattern of the model)
#CD > 1 or > 4/N

#create your model
model.cook <- lm(dat.no.uni$Amount ~ dat.no.uni$Need + dat.no.uni$Interest + dat.no.uni$Happy + dat.no.uni$Belief)

#find cook's distance for that model
dat.no.uni$cook <- cooks.distance(model.cook)

#create the cutoff
Cook.cutoff <- 4/nrow(dat.no.uni) 

# 4/98 --> cutoff = .04

#plot it out
plot(model.cook, which = 4, cook.levels = Cook.cutoff)

#Add a cutoff line
abline(h = Cook.cutoff, lty = 2) 


#Remove all outliers above your cutoff line
dat_final <- dat.no.uni[!(dat.no.uni$cook) > .04,]

#Removed 9 multivariate outliers (check n for dat.no.uni and compare with n for dat_final)

```


**HOMOSCEDASTICITY**
```{r}

#Breusch-Pagan test 
ncvTest(lm(dat_final$Amount ~ dat_final$Belief + dat_final$Need + dat_final$Interest + dat_final$Happy))

#not significant therefore not violated

#If violated utilize the Boxcox transformation

```


**MULTICOLLINEARITY**
```{r}

#Tolerance = 1 - R squared
#VIF = 1/Tolerance 
#  Small VIF values indicates low correlation among variables under ideal conditions
#Multicollinearity occurs when two or more predictors in the model are correlated and provide redundant information about the response. Multicollinearity was measured by variance inflation factors (VIF) and tolerance. If VIF value exceeding 4.0, or by tol- erance less than 0.2 then there is a problem with multicollinearity (Hair et al., 2010).

model2 <- linReg(data = dat_final, 
                 dep = 'Amount', 
                 cov = c('Belief', 'Need', 'Interest', 'Happy'),
                 blocks = list(c('Belief', 'Need', 'Interest', 'Happy')), 
                 modelTest = TRUE,
                 r2Adj = TRUE,
                 stdEst = TRUE,
                 ciStdEst = TRUE, 
                 collin = TRUE) #this line does the thing
model2

```


**Multiple Regression with not clean vs. clean data**
```{r}
# Multiple regression not clean
model1 <- linReg(data = dat, 
                 dep = 'Amount', 
                 covs = c('Belief', 'Need', 'Interest', 'Happy'),
                 blocks = list('Belief', 'Need', 'Interest', 'Happy'), 
                 modelTest = TRUE,
                 r2Adj = TRUE, 
                 stdEst = TRUE,
                 ciStdEst = TRUE)
model1

#Multiple regression clean
model3 <- linReg(data = dat_final, 
                 dep = 'Amount', 
                 covs = c('Belief', 'Need', 'Interest', 'Happy'),
                 blocks = list('Belief', 'Need', 'Interest', 'Happy'), 
                 modelTest = TRUE,
                 r2Adj = TRUE, 
                 stdEst = TRUE,
                 ciStdEst = TRUE)
model3
```

