---
title: "Linear Algebra for Data Science in R"
author: "DataCamp - Eric Eager"
date: "2/9/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(repos="https://CRAN.R-project.org")
```

## Motivations

![](_images/1871.png)

![](_images/1872.png)

![](_images/1873.png)

**Creating Vectors in R**

Vectors can be analyzed in a few different ways in R.

One way is to create the vector yourself, and there are a few different means by which you can do this.

The `rep()` command in R creates a vector that repeats the same element a prescribed number of times, while the `seq()` command creates a vector that follows a prescribed pattern.

A vector can be manually created by using the `c()` command, which stands for "concatenate".

```{r}
# Creating three 3's and four 4's, respectively
rep(3, 3)
rep(4, 4)

# Creating a vector with the first three even numbers and the first three odd numbers
seq(2, 6, by = 2)
seq(1, 5, by = 2)

# Re-creating the previous four vectors using the 'c' command
c(3, 3, 3)
c(4, 4, 4, 4)

c(2, 4, 6)
c(1, 3, 5)
```

Awesome! You've created vectors using some commands in R.

**The Algebra of Vectors**

Vectors can be made to make new vectors much in the same way that numbers can.

To add two vectors together in R, simply use the `+` command.

Vectors can also be scaled via multiplication by a real number (or scalar), using the `*` command.

Multiplication of two vectors is a trickier topic theoretically, but in R, component-wise multiplication of two vectors is legal and accomplished using the `*` command.

The vectors `x`, `y`, and `z` have been loaded for you.

```{r}
x <- c(1, 2, 3, 4, 5, 6, 7)
y <- c(2, 4, 6, 8, 10, 12, 14)
z <- c(1, 1, 2)

# Add x to y and print
print(x + y)

# Multiply z by 2 and print
print(2*z)

# Multiply x and y by each other and print
print(x*y)

# Add x to z, if possible, and print
print(x + z)
```

Great! Vector operations are a useful way to create other vectors. In the case where they are not the same size, R recycles elements of the shorter vector to obtain the given result (with a warning message).

**Creating Matrices in R**

Matrices can be created and analyzed in a few different ways in R.

One way is to create the matrix yourself. There are a few different ways you can do this.

The `matrix(a, nrow = b, ncol = c)` command in R creates a matrix that repeats the element `a` in a matrix with `b` rows and `c` columns.

A matrix can be manually created by using the `c()` command as well.

```{r}
A <- matrix(1, nrow = 2, ncol = 2)

# Create a matrix of all 1's and all 2's that are 2 by 3 and 3 by 2, respectively
matrix(1, nrow = 2, ncol = 3)

print(matrix(2, nrow = 3, ncol = 2))

# Create a matrix and changing the byrow designation.
B <- matrix(c(1, 2, 3, 2), nrow = 2, ncol = 2, byrow = FALSE)
B <- matrix(c(1, 2, 3, 2), nrow = 2, ncol = 2, byrow = TRUE)

# Add A to the previously-created matrix
A + B
```

Good work! You created matrices and added them to existing ones!

## Matrix-Vector Operations

![](_images/1874.png)

![](_images/1875.png)

![](_images/1876.png)

![](_images/1877.png)

**A vector needs to have the same amount of elements as the number of columns in a matrix for multiplication**

**Matrix Multiplication as a Transformation**

Matrices can be viewed as a way to transform collections of vectors into other vectors.

These transformations can take many forms, but the simplest ones in two dimensions are stretches or shrinkages (in either coordinate), reflections (e.g. about the x-axis, y-axis, origin, the line y = x), and rotations (clockwise, counter-clockwise).

Multiplication of a vector by a matrix is accomplished using the `%*%` command.

```{r}
A <- matrix(c(4, 0, 0, 1), 2, 2)
B <- matrix(c(1, 0, 0.0, 2/3), 2, 2)
b <- c(1,1)
A
B
b

# Multiply A by b
A%*%b

# Multiply B by b
B%*%b
```

You've seen how multiplication by a matrix can alter a vector.

**Reflections**

In the last exercise, you looked at stretching or shrinking components of a vector.

In this one, you'll apply a reflection matrix to the vector `b <- c(2, 1)`.

```{r}
A <- matrix(c(-1, 0, 0, 1), 2, 2)
B <- matrix(c(1, 0, 0, -1), 2, 2)
C <- matrix(c(-4, 0, 0, -2), 2, 2)
b <- c(2, 1)
A
B
C
b

# Multiply A by b 
A%*%b

# Multiply B by b 
B%*%b

# Multiply C by b
C%*%b
```

Awesome! You've seen how multiplication by a matrix can alter a vector.

## Matrix-Matrix Calculations

- order matters
  - matrix multiplication is not commutative

component-wise multiplication (`*`) is different from matrix multiplication (`%*%`)

![](_images/1878.png)

![](_images/1879.png)

- The identity matrix is commutative, however

![](_images/1880.png)

![](_images/1881.png)

**Matrix Multiplication - Order Matters**

In the last lesson, we studied how matrices act on vectors (stretches, shrinkages, reflections, rotations, etc.) and transform vectors into new vectors.

The successive application of these matrices can act as complex transformations, but because matrix multiplication is not commutative, the **order** of these transformations matter.

- The matrix with R output

```
> A
          [,1]       [,2]
[1,] 0.7071068 -0.7071068
[2,] 0.7071068  0.7071068
```

represents rotation of a 2-dimensional vector by 45 degrees counterclockwise.

- The matrix

```
> B
     [,1] [,2]
[1,]    1    0
[2,]    0   -1
```

represents a reflection about the x (first) axis.

```{r}
A <- matrix(c(0.7071068, 0.7071068, -0.7071068, 0.7071068), nrow = 2, ncol = 2)
B <- matrix(c(1, 0, 0, -1), nrow = 2, ncol = 2)
b <- c(1, 1)

# Multiply A by B
A%*%B

# Multiply A on the right of B
B%*%A

# Multiply the product of A and B by the vector b
A%*%B%*%b

# Multiply A on the right of B, and then by the vector b
B%*%A%*%b
```

Great Work! You've demonstrated that matrix multiplication is not commutative.

**Intro to The Matrix Inverse**

We talked briefly about the identity matrix in the video. Another important concept to understand in matrix multiplication is that of the matrix inverse.

For any number $a$ (aside from $0$), there's always a number $\frac{1}{a}$ that can be used to "undo" multiplication by $a$.

For matrices, this is not always true. However, when it is, we call the matrix that, when applied to $A$, yields the identity matrix $I$, that matrix's inverse.

The `solve` function in R will find the inverse of a matrix if it exists and provide an error if it does not.

```{r}
A <- matrix(c(1, -1, 2, 2), nrow = 2, ncol = 2)
A

# Take the inverse of the 2 by 2 identity matrix
solve(diag(2))

# Take the inverse of the matrix A
Ainv <- solve(A)

# Multiply A inverse by A
Ainv%*%A

# Multiply A by its inverse
A%*%Ainv
```

Awesome. You've discovered how to compute the inverse of a matrix, if it exists.

## Motivation for Solving Matrix-Vector Equations

A matrix multiplied by a vector is a linear combination of the columns of A times the elements of X.

![](_images/1882.png)

![](_images/1883.png)

![](_images/1884.png)

The elements of $\vec{x}$ are the coefficients when one creates $\vec{b}$ from the columns of $A$!
 
![](_images/1885.png)

**Exploring WNBA Data**

In this chapter, you will work with a matrix-vector model for team strength in the Women's National Basketball Association (WNBA) at the conclusion of the 2017 season. These team strengths can be used to predict who will win a match between any two teams, for example, using machine learning.

The WNBA has 12 teams, so your Massey Matrix $M$ will be 12x12, and will initially be loaded as a data frame. The vector with the point differentials for each team will be a vector with 12 elements.

```{r}
library(readr)
M <- read_csv("_data/WNBA_Data_2017_M.csv")
f <- read_csv("_data/WNBA_Data_2017_f.csv")

# Print the Massey Matrix M
print(M)

# Print the vector of point differentials f
print(f)

# Find the sum of the first column of M
sum(M[, 1])

# Find the sum of the vector f
sum(f)
```

Great! Notice that, since every point scored by one team is allowed by another, the vector f of point differentials needs to sum to zero!

## Matrix-Vector Equations - Some Theory

![](_images/1886.png)

![](_images/1887.png)

![](_images/1888.png)

![](_images/1889.png)

![](_images/1890.png)

![](_images/1891.png)

![](_images/1892.png)

The WNBA Massey matrix is not (computationally) invertible, so an adjustment needs to be made to our model.

One way we can change this is to add a row of `1`'s on the bottom of the matrix $M$, a column of `-1`'s to the far right of $M$, and a 0 to the bottom of the vector of point differentials $\vec{f}$.

In the setting of rating teams, that row of 1's represents (final equation stipulates) that The ratings for the entire league add to zero. This condition is good not only mathematically, but also for consistency in the rating system!

**Adjusting the Massey Matrix**

For our WNBA Massey Matrix model, some adjustments need to be made for a solution to our rating problem to exist and be unique. This is because the matrix we currently have is not (computationally) invertible.

In this exercise, you will actually add all of these to the matrix $M$.

```{r}
M <- read_csv("_data/WNBA_Data_2017_M.csv")
M <- as.matrix(M)

# Add a row of 1's
M_2 <- rbind(M, rep(1, 12))

# Add a column of -1's 
M_3 <- cbind(M_2, rep(-1, 13))

# Change the element in the lower-right corner of the matrix
M_3[13, 13] <- 1

# Print M_3
print(M_3)
```

Awesome! You're learning how to manipulate matrices in R!

**Inverting the Massey Matrix**

Now that you've updated the Massey Matrix $M$, you're almost ready to find out how the teams stacked up at the end of the 2017 season.

First, we need to make sure that the updated $M$ has an inverse.

```{r}
# Find the inverse of M
solve(M)
```

Awesome! You found the inverse of a real-world matrix!

## Solving Matrix-Vector Equations

![](_images/1893.png)

![](_images/1894.png)

![](_images/1895.png)

![](_images/1896.png)

**2017 WNBA Ratings!**

Now that we have our Massey matrix in a form for which is invertible, we can now use it to find the ratings for the WNBA teams at the conclusion of 2017!

`M` and an updated version of `f` are loaded for you.

```{r}
f <- as.matrix(f)

# Solve for r and rename column
r <- solve(M)%*%f
colnames(r) <- "Rating"

# Print r
print(r)
```

Awesome! You solved a matrix-vector equation in a real-world setting!

```{r}
library(dplyr)
as.data.frame(r) %>% arrange(desc(Rating))

```

The best team was Minnesota.

## Other Considerations for Matrix-Vector Equations

![](_images/1897.png)

![](_images/1898.png)

![](_images/1899.png)

![](_images/1900.png)

![](_images/1901.png)

**Alternatives to the Regular Matrix Inverse**

In the last video, we talked briefly about generalized or pseudo-inverses.

In this exercise, you'll use the Moore-Penrose generalized inverse in the `MASS' package and find that it produces the regular inverse if the matrix you're working with is already invertible!

The command `ginv()` computes the Moore-Penrose generalized inverse in R.

For more information on the Moore-Penrose inverse, see the following [link](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse).

```{r}
library(MASS)

# Print M
print(M)

# Find the rating vector the conventional way
r <- solve(M)%*%f
colnames(r) <- "Rating"
print(r)

# Find the rating vector using ginv
r <- ginv(M)%*%f
colnames(r) <- "Rating"
print(r)
```

Great work! You have found the final 2017 WNBA ratings in two ways!

## Intro to Eigenvalues and Eigenvectors

![](_images/1902.png)

![](_images/1903.png)

![](_images/1904.png)

![](_images/1905.png)

**Scaling Different Axes**

Suppose that you wanted to transform a two-dimensional vector so that the first element doubled in size, while the second element was cut by a third.

You can do this with matrix multiplication, as the matrix `A`, which yields the R output:

```r
     [,1]      [,2]
[1,]    2 0.0000000
[2,]    0 0.6666667
```

achieves the desired result.

```{r}
A <- matrix(c(2, 0, 0.00, 2/3), nrow = 2, ncol = 2)
# Multiply A by the given vector
print(A%*%c(1, 1))
```

Awesome! You've multiplied by a matrix that contracts and stretches elements of a vector!

## Definition of Eigenvalues and Eigenvectors

![](_images/1906.png)

![](_images/1907.png)

![](_images/1908.png)

Eigenvectors can be scaled (are about direction, not magnitude)

An eigenvector is a scalar multiple of itself ("own") when multiplied by $A$!

![](_images/1909.png)

**Finding Eigenvalues in R**

In the next video we will explicitly see how to find the eigenpairs for a matrix $A$, but right now we can at least show that a pair is an eigenpair for a matrix $A$. We can do this by showing that the difference between $A\vec{v}$ and $\lambda\vec{v}$ results in a vector of zeros.

The matrix `A` with R output:

```r
     [,1] [,2] [,3]
[1,]   -1    2    4
[2,]    0    7   12
[3,]    0    0   -4
```

is loaded for you.

```{r}
A <- matrix(c(-1,0,0,2,7,0,4,12,-4), nrow = 3, ncol = 3)

# Show that 7 is an eigenvalue for A
A%*%c(0.2425356, 0.9701425, 0) - 7*c(0.2425356, 0.9701425, 0)

# Show that -4 is an eigenvalue for A
A%*%c(-0.3789810, -0.6821657, 0.6253186) - -4*c(-0.3789810, -0.6821657, 0.6253186)

# Show that -1 is an eigenvalue for A
A%*%c(1, 0, 0) - -1*c(1, 0, 0)
```

Awesome! You've learned how to determine if a given pair is an eigenpair of a matrix. Note that there are often rounding errors that occur in R and, in this case, what you're seeing is an approximation of the zero vector.

**Scalar Multiplies of Eigenvectors are Eigenvectors**

As we said in the videos, an eigenvector of $A$ associated with a matrix $A$ can be scaled to meet the needs of the problem at hand. For example, for Markov models, having all of the elements sum to 1 means that the elements are probabilities, and thus have a clear interpretation.

In this exercise, we will be working with the first eigenpair in the previous exercise. For matrix $A$, this eigenpair has an eigenvalue $\lambda = 7$  and eigenvector:

```r
          [,1]
[1,] 0.2425356
[2,] 0.9701425
[3,] 0.0000000
```

```{r}
# Show that double an eigenvector is still an eigenvector
A%*%((2)*c(0.2425356, 0.9701425, 0)) - 7*(2)*c(0.2425356, 0.9701425, 0)

# Show half of an eigenvector is still an eigenvector
A%*%((0.5)*c(0.2425356, 0.9701425, 0)) - 7*(0.5)*c(0.2425356, 0.9701425, 0)
```

Awesome! You've shown that a scalar multiple of an eigenvector is still an eigenvector of a matrix! Note that often times there are rounding errors that occur in R, and in this case what you're seeing is an approximation of the zero vector.

## Computing Eigenvalues and Eigenvectors in R

![](_images/1910.png)

![](_images/1911.png)

![](_images/1912.png)

![](_images/1913.png)

**Verifying the Math on Eigenvalues**

In this exercise you'll find the eigenvalues $\lambda$ of a matrix $A$, and show that they satisfy the property that the matrix $\lambda I - A$ is not invertible, with determinant equal to zero.

```{r}
A <- matrix(c(1,1,2,1), nrow = 2, ncol = 2)

# Compute the eigenvalues of A and store in Lambda
Lambda <- eigen(A)

# Print eigenvalues
print(Lambda$values[1])
print(Lambda$values[2])

# Verify that these numbers satisfy the conditions of being an eigenvalue
det(Lambda$values[1]*diag(2) - A)
det(Lambda$values[2]*diag(2) - A)
```

Great! You showed that eigenvalues indeed satisfy the conditions they are meant to. Note that often times there are rounding errors that occur in R, and in this case what you're seeing is an approximation of the zero vector.

**Computing Eigenvectors in R**

In this exercise you'll find the eigenvectors of a matrix, and show that they satisfy the properties discussed in the lecture.

```{r}
# Find the eigenvectors of A and store them in Lambda
Lambda <- eigen(A)

# Print eigenvectors
print(Lambda$vectors[, 1])
print(Lambda$vectors[, 2])

# Verify that these eigenvectors & their associated eigenvalues satisfy lambda*v - A*v = 0
Lambda$values[1]*Lambda$vectors[, 1] - A%*%Lambda$vectors[, 1]
Lambda$values[2]*Lambda$vectors[, 2] - A%*%Lambda$vectors[, 2]
```

Awesome! You showed that eigenvectors indeed satisfy the conditions they are meant to. Note that often times there are rounding errors that occur in R, and in this case what you're seeing is an approximation of the zero vector.

## Some More on Eigenvalues and Eigenvectors

![](_images/1914.png)

![](_images/1915.png)

![](_images/1916.png)

**Eigenvalue Ordering**

R displays the eigenvalues in descending order of size because the largest eigenvalue(s) characterize the application of the matrix the more times it's applied.

**Markov Models for Allele Frequencies**

In the lecture, you saw that the leading eigenvalue of the Markov matrix $M$, whose R output is:

```r
      [,1]  [,2]  [,3]  [,4]
[1,] 0.980 0.005 0.005 0.010
[2,] 0.005 0.980 0.010 0.005
[3,] 0.005 0.010 0.980 0.005
[4,] 0.010 0.005 0.005 0.980
```

produced an eigenvector modeling a situation where the alleles are represented equally (each with probability 0.25).

In this exercise, we use a for-loop to iterate the process of mutation from an initial allele distribution of:

```r
[1] 1 0 0 0
```

and show that this is indeed what happens - that the eigenvector yields the right information in lieu of the for-loop.

For more on Markov Processes, see this [link](https://en.wikipedia.org/wiki/Markov_chain).

```{r}
# This code iterates mutation 1000 times
M <- matrix(c(.98,.005,.005,.01,.005,.98,.01,.005,.005,.01,.98,.005,.01,.005,.005,.98), nrow = 4, ncol = 4)
x <- c(1, 0, 0, 0)
for (j in 1:1000) {x <- M%*%x}

# Print x
print(x)

# Print and scale first eigenvector of M
Lambda <- eigen(M)
v1 <- Lambda$vectors[, 1]/sum(Lambda$vectors[, 1])

# Print v1
print(v1)
```

Great! You have seen how eigen analyses can help us understand genomics!

## Intro to the Idea of PCA

![](_images/1917.png)

![](_images/1918.png)

![](_images/1919.png)

![](_images/1920.png)

**Finding Redundancies**

One of the important things that principal component analysis can do is shrink redundancy in your dataset. In its simplest manifestation, redundancy occurs when two variables are correlated.

The Pearson correlation coefficient is a number between -1 and 1. Coefficients near zero indicate two variables are linearly independent, while coefficients near -1 or 1 indicate that two variables are linearly related.

The dataset `combine` has been loaded for you.

```{r}
combine <- read.csv("_data/DataCampCombine.csv", stringsAsFactors = TRUE)

# Print the first 6 observations of the dataset
head(combine, n = 6)

# Find the correlation between variables forty and three_cone
cor(combine$forty, combine$three_cone)

# Find the correlation between variables vertical and broad_jump
cor(combine$vertical, combine$broad_jump)
```

There are at least two redundancies in this dataset, and PCA can help us properly handle these.

## The Linear Algebra Behind PCA

![](_images/1921.png)

**First subtract mean from each column**

![](_images/1922.png)

![](_images/1923.png)

![](_images/1924.png)

In this example, the eigenvalue of the first column contains all of the information of the matrix. As we can see, the first column is simple 0.5 of the second column, they are perfectly correlated.

![](_images/1925.png)

**Standardizing Your Data**

In the lecture, we saw that you can learn a lot about a dataset by creating the matrix $A^TA$ from it. In this exercise, you'll do that with athletic data for players entering the National Football League college draft. The dataset `combine` is loaded for you.

```{r}
# Extract columns 5-12 of combine
A <- combine[, 5:12]

# Make A into a matrix
A <- as.matrix(A)

# Subtract the mean of each column
A <- apply(A, 2, function(x) x - mean(x))
```

Terrific! You've successfully pre-processed the `combine` dataset.

**Variance/Covariance Calculations**

In the last exercise you pre-processed the `combine` data into a matrix `A`. We now use `A` to create the *variance-covariance* matrix of this dataset.

```{r}
# Create matrix B from equation in instructions
B <- t(A)%*%A/(nrow(A) - 1)

# Compare 1st element of the 1st column of B to the variance of the first column of A
B[1,1]
var(A[, 1])

# Compare 1st element of 2nd column of B to the 1st element of the 2nd row of B to the covariance between the first two columns of A
B[1, 2]
B[2, 1]
cov(A[, 1], A[, 2])
```

Great stuff! You're understanding the structure of the variance-covariance matrix of a dataset.

**Eigenanalyses of Combine Data**

Now that you have looked at and understand the contents of `B`, you can study its eigen data to see if there is a chance to reduce the dimension of your data down from eight columns to something smaller. `B` is loaded for you.

```{r}
# Find eigenvalues of B
V <- eigen(B)

# Print eigenvalues
V$values
```

Great work. You've extracted a great deal of context about this dataset!

A great deal of the variability in the athletic profile of future NFL players can be attributed to one linear combination of the data!

## Performing PCA in R

![](_images/1926.png)

![](_images/1927.png)

![](_images/1928.png)

![](_images/1929.png)

![](_images/1930.png)

**Scaling Data Before PCA**

When dealing with data that has features with different scales, it's often important to scale the data first. This is because data that has larger values may sway the data even with relatively little variability.

The `combine` data frame is loaded for you.

```{r}
# Scale columns 5-12 of combine
B <- scale(combine[, 5:12])

# Print the first 6 rows of the data
head(B)

# Summarize the principal component analysis
summary(prcomp(B))
```

Awesome! You can perform PCA in R.

**Summarizing PCA in R**

As we saw in the video, there was a categorical variable (position) in our data that seemed to identify itself with clusters in the first two principal components. Even when scaling the data, these two PCs still explain a great deal of variation in the data. What if we looked at only one position at a time?

```{r}
# Subset combine only to "WR"
combine_WR <- subset(combine, position == "WR")

# Scale columns 5-12 of combine_WR
B <- scale(combine_WR[, 5:12])

# Print the first 6 rows of the data
head(B)

# Summarize the principal component analysis
summary(prcomp(B))
```

Once a major variable is removed, there's a lot more structure to the data's principal components!

Great work! You've used PCA to wrangle data, and then applied PCA again!

##Wrap-up

![](_images/1931.png)

![](_images/1932.png)

![](_images/1933.png)

![](_images/1934.png)

![](_images/1935.png)