---
title: "Cleaning Data in R"
author: "DataCamp - Maggie Matsui"
date: "12/11/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(repos="https://CRAN.R-project.org")
```

## Data type constraints

![](_images/671.png)

![](_images/672.png)

![](_images/673.png)

![](_images/674.png)

![](_images/675.png)

![](_images/676.png)

![](_images/677.png)

![](_images/678.png)

**Converting data types**

Throughout this chapter, you'll be working with San Francisco bike share ride data called `bike_share_rides`. It contains information on start and end stations of each trip, the trip duration, and some user information.

Before beginning to analyze any dataset, it's important to take a look at the different types of columns you'll be working with, which you can do using `glimpse()`.

In this exercise, you'll take a look at the data types contained in `bike_share_rides` and see how an incorrect data type can flaw your analysis.

`dplyr` and `assertive` are loaded and `bike_share_rides` is available.

```{r}
library(dplyr)
library(assertive)

bike_share_rides <- readRDS("_data/bike_share_rides_ch1_1.rds")

# Glimpse at bike_share_rides
glimpse(bike_share_rides)

# Summary of user_birth_year
summary(bike_share_rides$user_birth_year)

# Convert user_birth_year to factor: user_birth_year_fct
bike_share_rides <- bike_share_rides %>%
  mutate(user_birth_year_fct = as.factor(user_birth_year))

# Assert user_birth_year_fct is a factor
assert_is_factor(bike_share_rides$user_birth_year_fct)

# Summary of user_birth_year_fct
summary(bike_share_rides$user_birth_year_fct)
```

Dapper data type dexterity! Looking at the new summary statistics, more riders were born in `1988` than any other year.

**Trimming strings**

In the previous exercise, you were able to identify the correct data type and convert `user_birth_year` to the correct type, allowing you to extract counts that gave you a bit more insight into the dataset.

Another common dirty data problem is having extra bits like percent signs or periods in numbers, causing them to be read in as `characters`. In order to be able to crunch these numbers, the extra bits need to be removed and the numbers need to be converted from `character` to `numeric`. In this exercise, you'll need to convert the `duration` column from `character` to `numeric`, but before this can happen, the word `"minutes"` needs to be removed from each value.

`dplyr`, `assertive`, and `stringr` are loaded and `bike_share_rides` is available.

```{r}
library(stringr)

bike_share_rides <- bike_share_rides %>%
  # Remove 'minutes' from duration: duration_trimmed
  mutate(duration_trimmed = str_remove(duration, "minutes"),
         # Convert duration_trimmed to numeric: duration_mins
         duration_mins = as.numeric(duration_trimmed))

# Glimpse at bike_share_rides
glimpse(bike_share_rides)

# Assert duration_mins is numeric
assert_is_numeric(bike_share_rides$duration_mins)

# Calculate mean duration
mean(bike_share_rides$duration_mins)
```

Great work! By removing characters and converting to a numeric type, you were able to figure out that the average ride duration is about 13 minutes - not bad for a city like San Francisco!

## Range constraints

**What's an out of range value?**

- SAT score: 400-1600
- package weight: at least 0 lb/kg
- adult heart rate: 60-100 beats per minute

![](_images/679.png)

![](_images/680.png)

**Handling out of range values**

- remove rows
- treat as missing (`NA`)
- replace with range limit
- replace with other value based on domain knowledge and/orknowledge of dataset

![](_images/681.png)

![](_images/682.png)

![](_images/683.png)

![](_images/684.png)

![](_images/685.png)

**Ride duration constraints**

Values that are out of range can throw off an analysis, so it's important to catch them early on. In this exercise, you'll be examining the `duration_min` column more closely. Bikes are not allowed to be kept out for `more than 24 hours`(https://help.baywheels.com/hc/en-us/articles/360033790932-How-long-can-I-keep-a-bike-out-), or 1440 minutes at a time, but issues with some of the bikes caused inaccurate recording of the time they were returned.

In this exercise, you'll replace erroneous data with the range limit (1440 minutes), however, you could just as easily replace these values with `NA`s.

`dplyr`, `assertive`, and `ggplot2` are loaded and `bike_share_rides` is available.

```{r}
library(ggplot2)

# Create breaks
breaks <- c(min(bike_share_rides$duration_mins), 0, 1440, max(bike_share_rides$duration_mins))

# Create a histogram of duration_min
ggplot(bike_share_rides, aes(duration_mins)) +
  geom_histogram(breaks = breaks)

# Create breaks
breaks <- c(min(bike_share_rides$duration_mins), 0, 1440, max(bike_share_rides$duration_mins))

# Create a histogram of duration_min
ggplot(bike_share_rides, aes(duration_mins)) +
  geom_histogram(breaks = breaks)

# duration_min_const: replace vals of duration_min > 1440 with 1440
bike_share_rides <- bike_share_rides %>%
  mutate(duration_min_const = replace(duration_mins, duration_mins > 1440, 1440))

# Make sure all values of duration_min_const are between 0 and 1440
assert_all_are_in_closed_range(bike_share_rides$duration_min_const, lower = 0, upper = 1440)
```

Radical replacing! The method of replacing erroneous data with the range limit works well, but you could just as easily replace these values with `NA`s or something else instead.

**Back to the future**

Something has gone wrong and it looks like you have data with dates from the future, which is way outside of the date range you expected to be working with. To fix this, you'll need to remove any rides from the dataset that have a `date` in the future. Before you can do this, the `date` column needs to be converted from a character to a `Date`. Having these as `Date` objects will make it much easier to figure out which rides are from the future, since R makes it easy to check if one `Date` object is before (`<`) or after (`>`) another.

`dplyr` and `assertive` are loaded and `bike_share_rides` is available.

```{r}
library(lubridate)

# Convert date to Date type
bike_share_rides <- bike_share_rides %>%
  mutate(date = as.Date(date))

# Make sure all dates are in the past
assert_all_are_in_past(bike_share_rides$date)

# Filter for rides that occurred before or on today's date
bike_share_rides_past <- bike_share_rides %>%
  filter(date <= today())

# Make sure all dates from bike_share_rides_past are in the past
assert_all_are_in_past(bike_share_rides_past$date)
```

Fabulous filtering! Handling data from the future like this is much easier than trying to verify the data's correctness by time traveling.

## Uniqueness constraints

![](_images/686.png)

![](_images/687.png)

![](_images/688.png)

![](_images/689.png)

![](_images/690.png)

![](_images/691.png)

![](_images/692.png)

![](_images/693.png)

![](_images/694.png)

![](_images/695.png)

**Full duplicates**

You've been notified that an update has been made to the bike sharing data pipeline to make it more efficient, but that duplicates are more likely to be generated as a result. To make sure that you can continue using the same scripts to run your weekly analyses about ride statistics, you'll need to ensure that any duplicates in the dataset are removed first.

When multiple rows of a data frame share the same values for all columns, they're `full duplicates` of each other. Removing duplicates like this is important, since having the same value repeated multiple times can alter summary statistics like the mean and median. Each ride, including its `ride_id` should be unique.

`dplyr` is loaded and `bike_share_rides` is available.

```{r}
# Count the number of full duplicates
sum(duplicated(bike_share_rides))

# Remove duplicates
bike_share_rides_unique <- distinct(bike_share_rides)

# Count the full duplicates in bike_share_rides_unique
sum(duplicated(bike_share_rides_unique))
```

Dazzling duplicate removal! Removing full duplicates will ensure that summary statistics aren't altered by repeated data points.

**Removing partial duplicates**

Now that you've identified and removed the full duplicates, it's time to check for partial duplicates. Partial duplicates are a bit tricker to deal with than full duplicates. In this exercise, you'll first identify any partial duplicates and then practice the most common technique to deal with them, which involves dropping all partial duplicates, keeping only the first.

`dplyr` is loaded and `bike_share_rides` is available.

```{r}
# Find duplicated ride_ids
bike_share_rides %>% 
  # Count the number of occurrences of each ride_id
  count(ride_id) %>% 
  # Filter for rows with a count > 1
  filter(n > 1)

# Remove full and partial duplicates
bike_share_rides_unique <- bike_share_rides %>%
  # Only based on ride_id instead of all cols
  distinct(ride_id, .keep_all = TRUE)

# Find duplicated ride_ids in bike_share_rides_unique
bike_share_rides_unique %>% 
  # Count the number of occurrences of each ride_id
  count(ride_id) %>% 
  # Filter for rows with a count > 1
  filter(n > 1)
```

Perfect partial duplicate removing! It's important to consider the data you're working with before removing partial duplicates, since sometimes it's expected that there will be partial duplicates in a dataset, such as if the same customer makes multiple purchases.

**Aggregating partial duplicates**

Another way of handling partial duplicates is to compute a summary statistic of the values that differ between partial duplicates, such as mean, median, maximum, or minimum. This can come in handy when you're not sure how your data was collected and want an average, or if based on domain knowledge, you'd rather have too high of an estimate than too low of an estimate (or vice versa).

`dplyr` is loaded and `bike_share_rides` is available.

```{r}
bike_share_rides %>%
  # Group by ride_id and date
  group_by(ride_id, date) %>%
  # Add duration_min_avg column
  mutate(duration_min_avg = mean(duration_mins)) %>%
  # Remove duplicates based on ride_id and date, keep all cols
  distinct(ride_id, date, .keep_all = TRUE) %>%
  # Remove duration_min column
  select(-duration_mins)
```

Awesome aggregation! Aggregation of partial duplicates allows you to keep some information about all data points instead of keeping information about just one data point.

## Checking membership

![](_images/696.png)

![](_images/697.png)

![](_images/698.png)

![](_images/699.png)

![](_images/700.png)

![](_images/701.png)

![](_images/702.png)

![](_images/703.png)

![](_images/704.png)

![](_images/705.png)

**Not a member**

Now that you've practiced identifying membership constraint problems, it's time to fix these problems in a new dataset. Throughout this chapter, you'll be working with a dataset called `sfo_survey`, containing survey responses from passengers taking flights from San Francisco International Airport (SFO). Participants were asked questions about the airport's cleanliness, wait times, safety, and their overall satisfaction.

There were a few issues during data collection that resulted in some inconsistencies in the dataset. In this exercise, you'll be working with the `dest_size` column, which categorizes the size of the destination airport that the passengers were flying to. A data frame called `dest_sizes` is available that contains all the possible destination sizes. Your mission is to find rows with invalid `dest_sizes` and remove them from the data frame.

`dplyr` has been loaded and `sfo_survey` and `dest_sizes` are available.

```{r}
sfo_survey <- readRDS("_data/sfo_survey_ch2_1.rds")
dest_size <- c("Small", "Medium", "Large", "Hub")
passengers_per_day <- c("0-20K", "20K-70K", "70K-100K", "100K+")
dest_sizes <- data.frame(cbind(dest_size, passengers_per_day))
dest_sizes$passengers_per_day <- as.factor(dest_sizes$passengers_per_day)

# Count the number of occurrences of dest_size
sfo_survey %>%
  count(dest_size)

# Find bad dest_size rows
sfo_survey %>% 
  # Join with dest_sizes data frame to get bad dest_size rows
  anti_join(dest_sizes) %>%
  # Select id, airline, destination, and dest_size cols
  select(id, airline, destination, dest_size)

# Remove bad dest_size rows
sfo_survey %>% 
  # Join with dest_sizes
  semi_join(dest_sizes) %>%
  # Count the number of each dest_size
  count(dest_size)
```

Great joining! Anti-joins can help you identify the rows that are causing issues, and semi-joins can remove the issue-causing rows. In the next lesson, you'll learn about other ways to deal with bad values so that you don't have to lose rows of data.

## Categorical data problems

![](_images/706.png)

![](_images/707.png)

![](_images/708.png)

![](_images/709.png)

![](_images/710.png)

![](_images/711.png)

![](_images/712.png)

**Identifying inconsistency**

In the video exercise, you learned about different kinds of inconsistencies that can occur within categories, making it look like a variable has more categories than it should.

In this exercise, you'll continue working with the `sfo_survey` dataset. You'll examine the `dest_size` column again as well as the `cleanliness` column and determine what kind of issues, if any, these two categorical variables face.

`dplyr` is loaded and `sfo_survey` is available.

```{r}
# Count dest_size
sfo_survey %>%
  count(dest_size)

# Count cleanliness
sfo_survey %>%
  count(cleanliness)
```

**Correcting inconsistency**

Now that you've identified that `dest_size` has whitespace inconsistencies and `cleanliness` has capitalization inconsistencies, you'll use the new tools at your disposal to fix the inconsistent values in `sfo_survey` instead of removing the data points entirely, which could add bias to your dataset if more than 5% of the data points need to be dropped.

`dplyr` and `stringr` are loaded and `sfo_survey` is available.

```{r}
# Add new columns to sfo_survey
sfo_survey <- sfo_survey %>%
  # dest_size_trimmed: dest_size without whitespace
  mutate(dest_size_trimmed = str_trim(dest_size),
         # cleanliness_lower: cleanliness converted to lowercase
         cleanliness_lower = str_to_lower(cleanliness))

# Count values of dest_size_trimmed
sfo_survey %>%
  count(dest_size_trimmed)

# Count values of cleanliness_lower
sfo_survey %>%
  count(cleanliness_lower)
```

Lovely lowercase conversion and terrific trimming! You were able to convert seven-category data into four-category data, which will help your analysis go more smoothly.

**Collapsing categories**

One of the tablets that participants filled out the `sfo_survey` on was not properly configured, allowing the response for `dest_region` to be free text instead of a dropdown menu. This resulted in some inconsistencies in the `dest_region` variable that you'll need to correct in this exercise to ensure that the numbers you report to your boss are as accurate as possible.

`dplyr` and `forcats` are loaded and `sfo_survey` is available.

```{r}
library(forcats)

# Count categories of dest_region
sfo_survey %>%
  count(dest_region)

# Categories to map to Europe
europe_categories <- c("EU", "eur", "Europ")

# Add a new col dest_region_collapsed
sfo_survey %>%
  # Map all categories in europe_categories to Europe
  mutate(dest_region_collapsed = fct_collapse(dest_region, 
                                              Europe = europe_categories)) %>%
  # Count categories of dest_region_collapsed
  count(dest_region_collapsed)
```

Clean collapsing! You've reduced the number of categories from 12 to 9, and you can now be confident that 401 of the survey participants were heading to Europe.

## Cleaning text data

![](_images/713.png)

![](_images/714.png)

![](_images/715.png)

![](_images/716.png)

![](_images/717.png)

![](_images/718.png)

![](_images/719.png)

![](_images/720.png)

**Detecting inconsistent text data**

You've recently received some news that the customer support team wants to ask the SFO survey participants some follow-up questions. However, the auto-dialer that the call center uses isn't able to parse all of the phone numbers since they're all in different formats. After some investigation, you found that some phone numbers are written with hyphens (`-`) and some are written with parentheses (`(`,`)`). In this exercise, you'll figure out which phone numbers have these issues so that you know which ones need fixing.

`dplyr` and `stringr` are loaded, and `sfo_survey` is available.

```
# Filter for rows with "-" in the phone column
sfo_survey %>%
  filter(str_detect(phone, "-"))
  
# Filter for rows with "(" or ")" in the phone column
sfo_survey %>%
  filter(str_detect(phone, fixed("(")) | str_detect(phone, fixed(")")))
```

Delightful detection! Now that you've identified the inconsistencies in the `phone` column, it's time to remove unnecessary characters to make the follow-up survey go as smoothly as possible.

**Replacing and removing**

In the last exercise, you saw that the `phone` column of `sfo_data` is plagued with unnecessary parentheses and hyphens. The customer support team has requested that all phone numbers be in the format `"123 456 7890"`. In this exercise, you'll use your new `stringr` skills to fulfill this request.

`dplyr` and `stringr` are loaded and `sfo_survey` is available.

```
# Remove parentheses from phone column
phone_no_parens <- sfo_survey$phone %>%
  # Remove "("s
  str_remove_all(fixed("(")) %>%
  # Remove ")"s
  str_remove_all(fixed(")"))
   
# Add phone_no_parens as column
sfo_survey %>%
  mutate(phone_no_parens = phone_no_parens)
  
# Add phone_no_parens as column
sfo_survey %>%
  mutate(phone_no_parens = phone_no_parens,
  # Replace all hyphens in phone_no_parens with spaces
         phone_clean = str_replace_all(phone_no_parens, "-", " "))
```

Radical replacing and removing! Now that your phone numbers are all in a single format, the machines in the call center will be able to auto-dial the numbers, making it easier to ask participants follow-up questions.

**Invalid phone numbers**

The customer support team is grateful for your work so far, but during their first day of calling participants, they ran into some phone numbers that were invalid. In this exercise, you'll remove any rows with invalid phone numbers so that these faulty numbers don't keep slowing the team down.

`dplyr` and `stringr` are loaded and `sfo_survey` is available.

```
# Check out the invalid numbers
sfo_survey %>%
  filter(str_length(phone) != 12)

# Remove rows with invalid numbers
sfo_survey %>%
  filter(str_length(phone) == 12) %>% nrow()
```
```
sfo_survey %>%
  filter(str_length(phone) != 12)
    id   airline          destination    phone
1 2262    UNITED          BAKERSFIELD   0244 5
2 3081      COPA          PANAMA CITY 925 8846
3  340 SOUTHWEST              PHOENIX     1623
4 1128     DELTA MINNEAPOLIS-ST. PAUL  665 803
5  373    ALASKA    SAN JOSE DEL CABO    38515
```
```
[1] 2804
```

Mission accomplished! Thanks to your savvy string skills, the follow-up survey will be done in no time!


















