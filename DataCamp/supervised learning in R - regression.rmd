---
title: "Supervised Learning in R: Regression"
author: "DataCamp - Nina Zumel"
date: "12/20/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(repos="https://CRAN.R-project.org")
```

## Welcome and Introduction

**What is regression?**

**regression: predict a numerical outcome ("dependent variable") from a set of inputs ("independent variables").**

- *statistical sense*: predicting the expected value of the outcome.
- *casual sense*: predicting a numerical outcome, rather than a discrete one.

- *how many units will we sell?* (**Regression**)
- *will this customer buy our product (yes/no)?* (**Classification**)
- *what price will the customer pay for our product?* (**Regression**)

**Regression from a machine learning perspective**

- *scientific mindset*: modeling to understand the data generation process
  - *engineering mindset*: modeling to predict accurately
  
machine learning: engineering mindset

## Linear regression - the fundamental method

**Linear regression**

$y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \ldots$

- $y$ is *linearly* related to each $x_i$
- each $x_i$ contributes *additively* to $y$

**Linear regression in R: lm()**

```r
cmodel <- lm(temperature ~ chirps_per_sec, data = cricket)
```

- formula: `temperature ~ chirps_per_sec`
- data frame: `cricket`

**formulas**

- LHS: outcome
- RHS: inputs
  - use `+` for multiple inputs
  
`fmla_1 <- as.formula("temperature ~ chirps_per_sec")`

![](_images/1043.png)

For every unit increase in chirp rate, temperature should increase 3.291 degrees, everything else held constant.

![](_images/1044.png)

```r
broom::glance(cmodel)
sigr::wrapFTest(cmodel)
```

**Code a simple one-variable regression**

For the first coding exercise, you'll create a formula to define a one-variable modeling task, and then fit a linear model to the data. You are given the rates of male and female unemployment in the United States over several years ([Source](http://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/slr/frames/slr04.html)).

The task is to predict the rate of female unemployment from the observed rate of male unemployment. The outcome is `female_unemployment`, and the input is `male_unemployment`.

The sign of the variable coefficient tells you whether the outcome increases (+) or decreases (-) as the variable increases.

Recall the calling interface for [lm()](https://www.rdocumentation.org/packages/stats/topics/lm) is:

```r
lm(formula, data = ___)
```

```{r}
unemployment <- readRDS("_data/unemployment.rds")

# unemployment is loaded in the workspace
summary(unemployment)

# Define a formula to express female_unemployment as a function of male_unemployment
fmla <- female_unemployment ~ male_unemployment

# Print it
fmla

# Use the formula to fit a model: unemployment_model
unemployment_model <-  lm(fmla, data = unemployment)

# Print it 
unemployment_model
```

Good work. The coefficient for male unemployment is positive, so female unemployment increases as male unemployment does. Linear regression is the most basic of regression approaches. You can think of this course as ways to address its limitations.

**Examining a model**

Let's look at the model `unemployment_model` that you have just created. There are a variety of different ways to examine a model; each way provides different information. We will use [summary()](https://www.rdocumentation.org/packages/base/topics/summary), [broom::glance()](https://www.rdocumentation.org/packages/broom/topics/glance), and [sigr::wrapFTest()](https://www.rdocumentation.org/packages/sigr/topics/wrapFTest).

```{r}
library(broom)
library(sigr)

# broom and sigr are already loaded in your workspace
# Print unemployment_model
unemployment_model

# Call summary() on unemployment_model to get more details
summary(unemployment_model)

# Call glance() on unemployment_model to see the details in a tidier form
glance(unemployment_model)

# Call wrapFTest() on unemployment_model to see the most relevant details
wrapFTest(unemployment_model)
```

Great! There are several different ways to get diagnostics for your model. Use the one that suits your needs or preferences the best.

## Predicting once you fit a model

![](_images/1045.png)

**Predicting from the unemployment model**

In this exercise, you will use your unemployment model `unemployment_model` to make predictions from the `unemployment` data, and compare predicted female unemployment rates to the actual observed female unemployment rates on the training data, `unemployment`. You will also use your model to predict on the new data in `newrates`, which consists of only one observation, where male unemployment is 5%.

The [predict()](https://www.rdocumentation.org/packages/stats/topics/predict.lm) interface for lm models takes the form

```
predict(model, newdata)
```

You will use the `ggplot2` package to make the plots, so you will add the prediction column to the `unemployment` data frame. You will plot outcome versus prediction, and compare them to the line that represents perfect predictions (that is when the outcome is equal to the predicted value).

The `ggplot2` command to plot a scatterplot of `dframe$outcome` versus `dframe$pred` (`pred` on the *x* axis, `outcome` on the *y* axis), along with a blue line where `outcome == pred` is as follows:

```
ggplot(dframe, aes(x = pred, y = outcome)) + 
       geom_point() +  
       geom_abline(color = "blue")
```

```{r}
# unemployment is in your workspace
summary(unemployment)

# newrates is in your workspace
newrates <- data.frame(male_unemployment = 5)
newrates

# Predict female unemployment in the unemployment data set
unemployment$prediction <-  predict(unemployment_model)

# load the ggplot2 package
library(ggplot2)

# Make a plot to compare predictions to actual (prediction on x axis)
ggplot(unemployment, aes(x = prediction, y = female_unemployment)) + 
  geom_point() +
  geom_abline(color = "blue")

# Predict female unemployment rate when male unemployment is 5%
pred <- predict(unemployment_model, newdata = newrates)
# Print it
pred
```

Good job! While all the modeling algorithms in R implement the `predict()` method, the call may be a little different for each one.

**Multivariate linear regression (Part 1)**

In this exercise, you will work with the blood pressure dataset ([Source](http://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/mlr/frames/frame.html)), and model `blood_pressure` as a function of `weight` and `age`.

```{r}
bloodpressure <- readRDS("_data/bloodpressure.rds")

# bloodpressure is in the workspace
summary(bloodpressure)

# Create the formula and print it
fmla <- blood_pressure ~ age + weight
fmla

# Fit the model: bloodpressure_model
bloodpressure_model <- lm(fmla, data = bloodpressure)

# Print bloodpressure_model and call summary()
bloodpressure_model
summary(bloodpressure_model)
```

Good! One of the advantages of linear regression is that you can interpret the effects of each variable on the input â€“ to a certain extent. In this case the coefficients for both age and weight are positive, which indicates that blood pressure tends to increase as both age and weight increase.

**Multivariate linear regression (Part 2)**

Now you will make predictions using the blood pressure model `bloodpressure_model` that you fit in the previous exercise.

You will also compare the predictions to outcomes graphically. `ggplot2` is already loaded in your workspace. Recall the plot command takes the form:

```
ggplot(dframe, aes(x = pred, y = outcome)) + 
     geom_point() + 
     geom_abline(color = "blue")
```

```{r}
# bloodpressure is in your workspace
summary(bloodpressure)

# bloodpressure_model is in your workspace
bloodpressure_model

# predict blood pressure using bloodpressure_model :prediction
bloodpressure$prediction <- predict(bloodpressure_model)

# plot the results
ggplot(bloodpressure, aes(x = prediction, y = blood_pressure)) + 
    geom_point() +
    geom_abline(color = "blue")
```

Good! The results stay fairly close to the line of perfect prediction, indicating that the model fits the training data well. From a prediction perspective, multivariate linear regression behaves much as simple (one-variable) linear regression does.

## Wrapping up linear regression

**Pros and cons of linear regression**

- Pros
  - easy to fit and apply
  - concise
  - less prone to overfitting
  - interpretable
- Cons
  - can only express linear and additive relationships
  
**Collinearity**

- when variables are partially correlated
- coefficients may change sign
- high collinearity:
  - coefficients (or standard errors) look too large
  - model may be unstable
  
**Coming next**

- evaluating a regression model
- properly training a model

## Evaluating a model graphically

![](_images/1046.png)

![](_images/1047.png)

![](_images/1048.png)

![](_images/1049.png)

**Graphically evaluate the unemployment model**

In this exercise you will graphically evaluate the unemployment model, `unemployment_model`, that you fit to the `unemployment` data in the previous chapter. Recall that the model predicts `female_unemployment` from `male_unemployment`.

You will plot the model's predictions against the actual `female_unemployment`; recall the command is of the form

```r
ggplot(dframe, aes(x = pred, y = outcome)) + 
       geom_point() +  
       geom_abline()
```

Then you will calculate the residuals:

```r
residuals <- actual outcome - predicted outcome
```

and plot predictions against residuals. The residual graph will take a slightly different form: you compare the residuals to the horizontal line $x=0$  (using `geom_hline()`) rather than to the line $x = y$. The command will be provided.

The data frame `unemployment` and model `unemployment_model` are available in the workspace.

```{r}
# unemployment, unemployment_model are in the workspace
summary(unemployment)
summary(unemployment_model)

# Make predictions from the model
unemployment$predictions <- predict(unemployment_model)

# Fill in the blanks to plot predictions (on x-axis) versus the female_unemployment rates
ggplot(unemployment, aes(x = predictions, y = female_unemployment)) + 
  geom_point() + 
  geom_abline()

# From previous step
unemployment$predictions <- predict(unemployment_model)

# Calculate residuals
unemployment$residuals <- unemployment$female_unemployment - unemployment$predictions

# Fill in the blanks to plot predictions (on x-axis) versus the residuals
ggplot(unemployment, aes(x = predictions, y = residuals)) + 
  geom_pointrange(aes(ymin = 0, ymax = residuals)) + 
  geom_hline(yintercept = 0, linetype = 3) + 
  ggtitle("residuals vs. linear model prediction")
```

Congratulations! You have now evaluated model predictions by comparing them to ground truth, and by examining prediction error.

**The gain curve to evaluate the unemployment model**

In the previous exercise you made predictions about `female_unemployment` and visualized the predictions and the residuals. Now, you will also plot the gain curve of the `unemployment_model`'s predictions against actual `female_unemployment` using the [WVPlots::GainCurvePlot()](https://www.rdocumentation.org/packages/WVPlots/topics/GainCurvePlot) function.

For situations where order is more important than exact values, the gain curve helps you check if the model's predictions sort in the same order as the true outcome.

Calls to the function `GainCurvePlot()` look like:

```r
GainCurvePlot(frame, xvar, truthvar, title)
```

where

- `frame` is a data frame
- `xvar` and `truthvar` are strings naming the prediction and actual outcome columns of frame
- `title` is the title of the plot

When the predictions sort in exactly the same order, the relative Gini coefficient is 1. When the model sorts poorly, the relative Gini coefficient is close to zero, or even negative.

```{r}
# unemployment is in the workspace (with predictions)
summary(unemployment)

# unemployment_model is in the workspace
summary(unemployment_model)

# Load the package WVPlots
library(WVPlots)

# Plot the Gain Curve
GainCurvePlot(unemployment, "predictions", "female_unemployment", "Unemployment model")
```

Congratulations! A relative gini coefficient close to one shows that the model correctly sorts high unemployment situations from lower ones.

## Root Mean Squared Error (RMSE)

**What is Root Mean Squared Error (RMSE)?**

$RMSE = \sqrt{\overline{(pred-y)^2}}$

where
- $pred - y$: the error, or residuals vector
- $\overline{(pred-y)^2}$: mean value of $(pred - y)^2$

![](_images/1050.png)

![](_images/1051.png)

$RMSE < sd_{(outcome)}$ indicates that the model performs better than simply taking the average of the outcome variable

**Calculate RMSE**

In this exercise you will calculate the RMSE of your unemployment model. In the previous coding exercises, you added two columns to the `unemployment` dataset:

- the model's predictions (`predictions` column)
- the residuals between the predictions and the outcome (`residuals` column)

You can calculate the RMSE from a vector of residuals, $res$, as:

$RMSE = \sqrt{mean(res^2)}$

You want RMSE to be small. How small is "small"? One heuristic is to compare the RMSE to the standard deviation of the outcome. With a good model, the RMSE should be smaller.

```{r}
# unemployment is in the workspace
summary(unemployment)

# For convenience put the residuals in the variable res
res <- unemployment$residuals

# Calculate RMSE, assign it to the variable rmse and print it
(rmse <- sqrt(mean(res^2)))

# Calculate the standard deviation of female_unemployment and print it
(sd_unemployment <- sd(unemployment$female_unemployment))
```

Good job! An RMSE much smaller than the outcome's standard deviation suggests a model that predicts well.

## R-Squared

**What is $R^2$?**

A measure of how well the model fits or explains the data

- a value between 0-1
  - near 1: model fits well
  - near 0: no better than guessing the average value
  
**Calculating $R^2$**

$R^2$ is the *variance explained by the model*

$R^2 = 1 - \frac{RSS}{SS_{Tot}}$

where

- $RSS = \sum(y - prediction)^2$
  - residual sum of squares (variance from model)
  
- $SS_{Tot} = \sum(y-\bar{y})^2$
  - total sum of squares (variance of data)

**Correlation and R^2**

- square of the correlation between outcome and predicted value $= R^2$
- true for models that minimize squared error:
  - linear regression
  - GAM regression
  - tree-based algorithms that minimize squared error
- true for training data; **NOT** true for future application data

**Calculate R-Squared**

Now that you've calculated the RMSE of your model's predictions, you will examine how well the model fits the data: that is, how much variance does it explain. You can do this using $R^2$.

Suppose $y$ is the true outcome, $p$ is the prediction from the model, and $res = y - p$ are the residuals of the predictions.

Then the total sum of squares $tss$ ("total variance") of the data is:

$tss = \sum (y - \bar{y})^2$

where $\bar{y}$ is the mean value of $y$.

The residual sum of squared errors of the model, $rss$ is:

$rss = \sum res^2$

$R^2$ (R-Squared), the "variance explained" by the model, is then:

 $1 - \frac{rss}{tss}$

After you calculate $R^2$, you will compare what you computed with the $R^2$ reported by [glance()](https://www.rdocumentation.org/packages/broom/topics/glance). `glance()` returns a one-row data frame; for a linear regression model, one of the columns returned is the $R^2$ of the model on the training data.

The data frame `unemployment` is in your workspace, with the columns `predictions` and `residuals` that you calculated in a previous exercise.

```{r}
# unemployment is in your workspace
summary(unemployment)

# unemployment_model is in the workspace
summary(unemployment_model)

# Calculate mean female_unemployment: fe_mean. Print it
(fe_mean <- mean(unemployment$female_unemployment))

# Calculate total sum of squares: tss. Print it
(tss <- sum( (unemployment$female_unemployment - fe_mean)^2 ))

# Calculate residual sum of squares: rss. Print it
(rss <- sum(unemployment$residuals^2))

# Calculate R-squared: rsq. Print it. Is it a good fit?
(rsq <- 1 - (rss/tss))

# Get R-squared from glance. Print it
(rsq_glance <- glance(unemployment_model)$r.squared)
```

Excellent! An R-squared close to one suggests a model that predicts well.

**Correlation and R-squared**

The linear correlation of two variables, $x$ and $y$, measures the strength of the linear relationship between them. When $x$ and $y$ are respectively:

- the outcomes of a regression model that minimizes squared-error (like linear regression) and
- the true outcomes of *the training data*,

then the square of the correlation is the same as $R^2$. You will verify that in this exercise.

```{r}
# unemployment is in your workspace
summary(unemployment)

# unemployment_model is in the workspace
summary(unemployment_model)

# Get the correlation between the prediction and true outcome: rho and print it
(rho <- cor(unemployment$predictions, unemployment$female_unemployment))

# Square rho: rho2 and print it
(rho2 <- rho ^ 2)

# Get R-squared from glance and print it
(rsq_glance <- glance(unemployment_model)$r.squared)
```

Success! Remember this equivalence is only true for the training data, and only for models that minimize squared error.

## Properly Training a Model

**Models can perform much better on training than they do on future data**

![](_images/1052.png)

![](_images/1053.png)

![](_images/1054.png)

![](_images/1055.png)

![](_images/1056.png)

![](_images/1057.png)

![](_images/1058.png)

![](_images/1059.png)

**Generating a random test/train split**

For the next several exercises you will use the `mpg` data from the package `ggplot2`. The data describes the characteristics of several makes and models of cars from different years. The goal is to predict city fuel efficiency from highway fuel efficiency.

In this exercise, you will split `mpg` into a training set `mpg_train` (75% of the data) and a test set `mpg_test` (25% of the data). One way to do this is to generate a column of uniform random numbers between 0 and 1, using the function [runif()](https://www.rdocumentation.org/packages/stats/topics/Uniform).

If you have a data set `dframe` of size $N$, and you want a random subset of approximately size $100 * X\%$ of $N$ (where $X$ is between 0 and 1), then:

1. Generate a vector of uniform random numbers: `gp = runif(N)`.
2. `dframe[gp < X,]` will be about the right size.
3. `dframe[gp >= X,]` will be the complement.

```{r}
# mpg is in the workspace
summary(mpg)
dim(mpg)

# Use nrow to get the number of rows in mpg (N) and print it
(N <- nrow(mpg))

# Calculate how many rows 75% of N should be and print it
# Hint: use round() to get an integer
(target <- round(N * 0.75))

# Create the vector of N uniform random variables: gp
gp <- runif(N)

# Use gp to create the training set: mpg_train (75% of data) and mpg_test (25% of data)
mpg_train <- mpg[gp < 0.75, ]
mpg_test <- mpg[gp >= 0.75, ]

# Use nrow() to examine mpg_train and mpg_test
nrow(mpg_train)
nrow(mpg_test)
```

Great job! A random split won't always produce sets of exactly X% and (100-X)% of the data, but it should be close.

**Train a model using test/train split**

Now that you have split the `mpg` dataset into `mpg_train` and `mpg_test`, you will use `mpg_train` to train a model to predict city fuel efficiency (`cty`) from highway fuel efficiency (`hwy`).

```{r}
# mpg_train is in the workspace
summary(mpg_train)

# create a formula to express cty as a function of hwy: fmla and print it.
(fmla <- cty ~ hwy)

# Now use lm() to build a model mpg_model from mpg_train that predicts cty from hwy 
mpg_model <- lm(fmla, data = mpg_train)

# Use summary() to examine the model
summary(mpg_model)
```

Great! Now it's time to apply the model.

**Evaluate a model using test/train split**

Now you will test the model `mpg_model` on the test data, `mpg_test`. Functions `rmse()` and `r_squared()` to calculate RMSE and R-squared have been provided for convenience:

```
rmse(predcol, ycol)
r_squared(predcol, ycol)
```

where:

- `predcol`: The predicted values
- `ycol`: The actual outcome

You will also plot the predictions vs. the outcome.

Generally, model performance is better on the training data than the test data (though sometimes the test set "gets lucky"). A slight difference in performance is okay; if the performance on training is significantly better, there is a problem.

```{r}
rmse <- function(predcol, ycol) {
  res = predcol-ycol
  sqrt(mean(res^2))
}

r_squared <- function(predcol, ycol) {
  tss = sum( (ycol - mean(ycol))^2 )
  rss = sum( (predcol - ycol)^2 )
  1 - rss/tss
}

# Examine the objects in the workspace
ls.str()

# predict cty from hwy for the training set
mpg_train$pred <- predict(mpg_model)

# predict cty from hwy for the test set
mpg_test$pred <- predict(mpg_model, newdata = mpg_test)

# Evaluate the rmse on both training and test data and print them
(rmse_train <- rmse(mpg_train$pred, mpg_train$cty))
(rmse_test <- rmse(mpg_test$pred, mpg_test$cty))


# Evaluate the r-squared on both training and test data.and print them
(rsq_train <- r_squared(mpg_train$pred, mpg_train$cty))
(rsq_test <- r_squared(mpg_test$pred, mpg_test$cty))

# Plot the predictions (on the x-axis) against the outcome (cty) on the test data
ggplot(mpg_test, aes(x = pred, y = cty)) + 
  geom_point() + 
  geom_abline()
```

Excellent! Good performance on the test data is more confirmation that the model works as expected.

**Create a cross validation plan**

There are several ways to implement an n-fold cross validation plan. In this exercise you will create such a plan using [vtreat::kWayCrossValidation()](https://www.rdocumentation.org/packages/vtreat/versions/topics/kWayCrossValidation), and examine it.

`kWayCrossValidation()` creates a cross validation plan with the following call:

```r
splitPlan <- kWayCrossValidation(nRows, nSplits, dframe, y)
```

where `nRows` is the number of rows of data to be split, and `nSplits` is the desired number of cross-validation folds.

Strictly speaking, `dframe` and `y` aren't used by `kWayCrossValidation`; they are there for compatibility with other `vtreat` data partitioning functions. You can set them both to `NULL`.

The resulting `splitPlan` is a list of `nSplits` elements; each element contains two vectors:

- `train`: the indices of `dframe` that will form the training set
- `app`: the indices of `dframe` that will form the test (or application) set

In this exercise you will create a 3-fold cross-validation plan for the data set `mpg`.

```{r}
# Load the package vtreat
library(vtreat)

# mpg is in the workspace
summary(mpg)

# Get the number of rows in mpg
nRows <- nrow(mpg)

# Implement the 3-fold cross-fold plan with vtreat
splitPlan <- kWayCrossValidation(nRows, 3, NULL, NULL)

# Examine the split plan
str(splitPlan)
```

Congratulations! You have created a 3-way cross validation plan. In the next exercise you will use this plan to evaluate a potential model.

**Evaluate a modeling procedure using n-fold cross-validation**

In this exercise you will use `splitPlan`, the 3-fold cross validation plan from the previous exercise, to make predictions from a model that predicts `mpg$cty` from `mpg$hwy`.

If `dframe` is the training data, then one way to add a column of cross-validation predictions to the frame is as follows:

```r
# Initialize a column of the appropriate length
dframe$pred.cv <- 0 

# k is the number of folds
# splitPlan is the cross validation plan

for(i in 1:k) {
  # Get the ith split
  split <- splitPlan[[i]]

  # Build a model on the training data 
  # from this split 
  # (lm, in this case)
  model <- lm(fmla, data = dframe[split$train,])

  # make predictions on the 
  # application data from this split
  dframe$pred.cv[split$app] <- predict(model, newdata = dframe[split$app,])
}
```

Cross-validation predicts how well a model built from all the data will perform on new data. As with the test/train split, for a good modeling procedure, cross-validation performance and training performance should be close.

```{r}
# mpg is in the workspace
summary(mpg)

# splitPlan is in the workspace
str(splitPlan)

# Run the 3-fold cross validation plan from splitPlan
k <- 3 # Number of folds
mpg$pred.cv <- 0 
for(i in 1:k) {
  split <- splitPlan[[i]]
  model <- lm(cty ~ hwy, data = mpg[split$train, ])
  mpg$pred.cv[split$app] <- predict(model, newdata = mpg[split$app, ])
}

# Predict from a full model
mpg$pred <- predict(lm(cty ~ hwy, data = mpg))

# Get the rmse of the full model's predictions
rmse(mpg$pred, mpg$cty)

# Get the rmse of the cross-validation predictions
rmse(mpg$pred.cv, mpg$cty)
```

Congratulations! You have successfully estimated a model's out-of-sample error via cross-validation. Remember, cross-validation validates the *modeling process*, not an actual model.













