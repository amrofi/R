---
title: "time series analysis in R"
author: "Daniel Pinedo"
date: "10/25/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Welcome to the course!**

* Introduction
  + Time Series: A sequence of data in chronological order.
  + Data is commonly recorded sequentially, over time.
  + Time series data is everywhere
* Time series data
  + Time series data is dated or time stamped in R
  + `print(Time_Series)`
* Time series plots
  + `plot(Time_Series)`
  + time is indexed on horizontal axis
  + observations in time from first on left to last on right
  + line connects neighboring observations
* Basic time series models
  + White Noise (WN)
  + Random Walk (RW)
  + Autoregression (AR)
  + Simple Moving Average (MA)


**Exploring raw time series**

The most common first step when conducting time series analysis is to display your time series dataset in a visually intuitive format. The most useful way to view raw time series data in R is to use the **print()** command, which displays the `Start`, `End`, and `Frequency` of your data along with the observations.

Another useful command for viewing time series data in R is the **length()** function, which tells you the total number of observations in your data.

Some datasets are very long, and previewing a subset of data is more suitable than displaying the entire series. The `head(, n =)` and `tail(, n =)` functions, in which `n` is the number of items to display, focus on the first and last few elements of a given dataset respectively.

In this exercise, you'll explore the famous River Nile annual streamflow data, `Nile`. This time series dataset includes some metadata information. When calling `print(Nile)`, note that `Start = 1871` indicates that 1871 is the year of the first annual observation, and `End = 1970` indicates 1970 is the year of the last annual observation.
```{r}
# Print the Nile dataset
print(Nile)

# List the number of observations in the Nile dataset
length(Nile)

# Display the first 10 elements of the Nile dataset
head(Nile, n = 10)

# Display the last 12 elements of the Nile dataset
tail(Nile, n = 12)
```

As you can see, these commands are very useful when exploring raw time series data.

**Basic time series plots**

While simple commands such as `print()`, `length()`, `head()`, and `tail()` provide crucial information about your time series data, another very useful way to explore any data is to generate a plot.

In this exercise, you will plot the River Nile annual streamflow data using the **plot()** function. For time series data objects such as `Nile`, a `Time` index for the horizontal axis is typically included. From the previous exercise, you know that this data spans from 1871 to 1970, and horizontal tick marks are labeled as such. The default label of "Time" is not very informative. Since these data are annual measurements, you should use the label "Year". While you're at it, you should change the vertical axis label to "River Volume (1e9 m^{3})".

Additionally, it helps to have an informative title, which can be set using the argument `main`. For your purposes, a useful title for this figure would be "Annual River Nile Volume at Aswan, 1871-1970".

Finally, the default plotting `type` for time series objects is `"l"` for line. Connecting consecutive observations can help make a time series plot more interpretable. Sometimes it is also useful to include both the observations points as well as the lines, and we instead use `"b" `for both.
```{r}
# Plot the Nile data
plot(Nile)

# Plot the Nile data with xlab and ylab arguments
plot(Nile, xlab = "Year", ylab = "River Volume (1e9 m^{3})")

# Plot the Nile data with xlab, ylab, main, and type arguments
plot(Nile, xlab = "Year", ylab = "River Volume (1e9 m^{3})", main = "Annual River Nile Volume at Aswan, 1871-1970", type = "b")

```

The `plot()` command is one of the most versatile commands in R. When used with time series data, this command automatically plots your data over time.

**What does the time index tell us?**

Some data are naturally evenly spaced by time. The time series `discrete_data` shown in the top figure has 20 observations, with one observation appearing at each of the discrete time indices 1 through 20. Discrete time indexing is appropriate for `discrete_data`.

The time series `continuous_series` shown in the bottom figure also has 20 observations, it is following the same periodic pattern as `discrete_data`, but its observations are not evenly spaced. Its first, second, and last observations were observed at times 1.210322, 1.746137, and 20.180524, respectively. Continuous time indexing is natural for `continuous_series`, however, the observations are approximately evenly spaced, with about 1 observation observed per time unit. Let's investigate using a discrete time indexing for `continuous_series`.

![1.png](_images/1.png)
![2.png](_images/2.png)
![3.png](_images/3.png)
```{r}
continuous_time_index <- c(1.210322, 1.746137,  2.889634,  3.591384,  5.462065,  5.510933,  7.074295, 8.264398,  9.373382,
                           9.541063, 11.161122, 12.378371, 13.390559, 14.066280, 15.093547, 15.864515, 16.857413, 18.091457,                            19.365451, 20.180524)

continuous_series <- c(0.56889468, 0.76630408, 0.99207512, 0.97481741, 0.39912320, 0.37660246, -0.38532033, -0.83635852, 
                       -0.99966983, -0.99831019, -0.64622280, -0.09386151, 0.40052909,  0.68160578,  0.95318159,  
                       0.99693803,  0.83934194,  0.37003754, -0.25509676, -0.61743983)

# Plot the continuous_series using continuous time indexing
par(mfrow=c(2,1))
plot(continuous_time_index, continuous_series, type = "b")

# Make a discrete time index using 1:20 
discrete_time_index <- 1:20

# Now plot the continuous_series using discrete time indexing
plot(discrete_time_index, continuous_series, type = "b")

```
**Sampling frequency**

- Basic simplifying assumptions for time series:
  - consecutive observations are equally spaced
  - apply discrete-time observation index
  - this may only hold approximatetely
- R functions:
  - `start()`
  - `end()`
  - `frequency()`
  - `deltat()`


**Identifying the sampling frequency**

In addition to viewing your data and plotting over time, there are several additional operations that can be performed on time series datasets.

The `start()` and `end()` functions return the time index of the first and last observations, respectively. The `time()` function calculates a vector of time indices, with one element for each time index on which the series was observed.

The `deltat()` function returns the fixed time interval between observations and the `frequency()` function returns the number of observations per unit time. Finally, the `cycle()` function returns the position in the cycle of each observation.

In this exercise, you'll practice applying these functions to the `AirPassengers` dataset, which reports the monthly total international airline passengers (in thousands) from 1949 to 1960.
```{r}
# Plot AirPassengers
plot(AirPassengers)

# View the start and end dates of AirPassengers
start(AirPassengers)
end(AirPassengers)

# Use time(), deltat(), frequency(), and cycle() with AirPassengers 
deltat(AirPassengers)
frequency(AirPassengers)
time(AirPassengers)
cycle(AirPassengers)
```

These commands provide considerable descriptive information about the structures and patterns in your time series data. It may help to keep these commands handy when working with your own time series data.

**Missing values**

Sometimes there are missing values in time series data, denoted `NA` in R, and it is useful to know their locations. It is also important to know how missing values are handled by various R functions. Sometimes we may want to ignore any missingness, but other times we may wish to impute or estimate the missing values.

Let's again consider the monthly `AirPassengers `dataset, but now the data for the year 1956 are missing. In this exercise, you'll explore the implications of this missing data and impute some new data to solve the problem.

The `mean()` function calculates the sample mean, but it fails in the presence of any `NA` values. Use `mean(, na.rm = TRUE)` to calculate the mean with all missing values removed. It is common to replace missing values with the mean of the observed values. Does this simple data imputation scheme appear adequate when applied the the `AirPassengers` dataset?
```{r}

# Plot the AirPassengers data
plot(AirPassengers, type = "l")

# Compute the mean of AirPassengers
mean(AirPassengers, na.rm = TRUE)

# Impute mean values to NA in AirPassengers
AirPassengers[85:96] <- mean(AirPassengers, na.rm = TRUE)

# Generate another plot of AirPassengers
plot(AirPassengers)

# Add the complete AirPassengers data to your plot
rm(AirPassengers)
points(AirPassengers, type = "l", col = 2, lty = 3)

```

ased on your plot, it seems that simple data imputation using the mean is not a great method to approximate what's really going on in the AirPassengers data.

**Basic time series objects**

- Building ts() objects
  - start with a vector of data
  - apply the `ts()` function
  - specify the start date and observation frequency
    - `time_series <- ts(data_vector, start = 2001, frequency = 1)`
- Why ts() objects?
  - improved plotting
  - access to time index information
  - model estimation and forecasting (later chapters)


**Creating a time series object with ts()**

The function [ts()](https://www.rdocumentation.org/packages/stats/versions/3.3.1/topics/ts) can be applied to create time series objects. A time series object is a vector (univariate) or matrix (multivariate) with additional attributes, including time indices for each observation, the sampling frequency and time increment between observations, and the cycle length for periodic data. Such objects are of the `ts` class, and represent data that has been observed at (approximately) equally spaced time points. Now you will create time series objects yourself.

The advantage of creating and working with time series objects of the `ts` class is that many methods are available for utilizing time series attributes, such as time index information. For example, as you've seen in earlier exercises, calling `plot()` on a ts object will automatically generate a plot over time.

In this exercise, you'll familiarize yourself with the `ts` class by encoding some time series data (saved as `data_vector`) into `ts` and exploring the result. Your time series `data_vector` starts in the year 2004 and has 4 observations per year (i.e. it is *quarterly* data).
```
# Use print() and plot() to view data_vector
print(data_vector)
plot(data_vector)

######################################################
[1]  2.0521941073  4.2928852797  3.3294132944  3.5085950670  0.0009576938
 [6]  1.9217186345  0.7978134128  0.2999543435  0.9435687536  0.5748283388
[11] -0.0034005903  0.3448649176  2.2229761136  0.1763144576  2.7097622770
[16]  1.2501948965 -0.4007164754  0.8852732121 -1.5852420266 -2.2829278891
[21] -2.5609531290 -3.1259963754 -2.8660295895 -1.7847009207 -1.8894912908
[26] -2.7255351194 -2.1033141800 -0.0174256893 -0.3613204151 -2.9008403327
[31] -3.2847440927 -2.8684594718 -1.9505074437 -4.8801892525 -3.2634605353
[36] -1.6396062522 -3.3012575840 -2.6331245433 -1.7058354022 -2.2119825061
[41] -0.5170595186  0.0752508095 -0.8406994716 -1.4022683487 -0.1382114230
[46] -1.4065954703 -2.3046941055  1.5073891432  0.7118679477 -1.1300519022
######################################################

# Convert data_vector to a ts object with start = 2004 and frequency = 4
time_series <- ts(data_vector, start = 2004, frequency = 4)

# Use print() and plot() to view time_series
print(time_series)
plot(time_series)

######################################################
print(time_series)
              Qtr1          Qtr2          Qtr3          Qtr4
2004  2.0521941073  4.2928852797  3.3294132944  3.5085950670
2005  0.0009576938  1.9217186345  0.7978134128  0.2999543435
2006  0.9435687536  0.5748283388 -0.0034005903  0.3448649176
2007  2.2229761136  0.1763144576  2.7097622770  1.2501948965
2008 -0.4007164754  0.8852732121 -1.5852420266 -2.2829278891
2009 -2.5609531290 -3.1259963754 -2.8660295895 -1.7847009207
2010 -1.8894912908 -2.7255351194 -2.1033141800 -0.0174256893
2011 -0.3613204151 -2.9008403327 -3.2847440927 -2.8684594718
2012 -1.9505074437 -4.8801892525 -3.2634605353 -1.6396062522
2013 -3.3012575840 -2.6331245433 -1.7058354022 -2.2119825061
2014 -0.5170595186  0.0752508095 -0.8406994716 -1.4022683487
2015 -0.1382114230 -1.4065954703 -2.3046941055  1.5073891432
2016  0.7118679477 -1.1300519022                
######################################################
```
![4.png](_images/4.png)
![5.png](_images/5.png)
As you can see, `ts` objects are treated differently by commands such as `print()` and `plot()`. For example, automatic use of the time-index in your calls to `plot()` requires a `ts` object.

**Testing whether an object is a time series**

When you work to create your own datasets, you can build them as `ts` objects. Recall the dataset `data_vector` from the previous exercise, which was just a vector of numbers, and `time_series`, the `ts` object you created from `data_vector` using the `ts()` function and information regarding the start time and the observation frequency. As a reminder, `data_vector` and `time_series` are shown in the plots above.

When you use datasets from others, such as those included in an R package, you can check whether they are `ts` objects using the **is.ts()** command. The result of the test is either `TRUE` when the data is of the `ts` class, or `FALSE` if it is not.

In this exercise, you'll explore the class of the datasets you've been using throughout this chapter.

```
# Check whether data_vector and time_series are ts objects
is.ts(data_vector)
is.ts(time_series)

######################################################
[1] FALSE
######################################################
```
```{r}
# Check whether Nile is a ts object
is.ts(Nile)

# Check whether AirPassengers is a ts object
is.ts(AirPassengers)
```
`is.ts()` is a simple command for determining whether or not you're working with a `ts` object. As you can see, the `Nile` and `AirPassengers` datasets you worked with earlier in the chapter are both encoded as `ts` objects.

**Plotting a time series object**

It is often very useful to plot data we are analyzing, as is the case when conducting time series analysis. If the dataset under study is of the `ts` class, then the `plot()` function has methods that automatically incorporate time index information into a figure.

Let's consider the `eu_stocks` dataset (available in R by default as `EuStockMarkets`). This dataset contains daily closing prices of major European stock indices from 1991-1998, specifically, from Germany (`DAX`), Switzerland (`SMI`), France (`CAC`), and the UK (`FTSE`). The data were observed when the markets were open, so there are no observations on weekends and holidays. We will proceed with the approximation that this dataset has evenly spaced observations and is a four dimensional time series.

To conclude this chapter, this exercise asks you to apply several of the functions you've already learned to this new dataset.
```{r}
eu_stocks <- EuStockMarkets

# Check whether eu_stocks is a ts object
is.ts(eu_stocks)

# View the start, end, and frequency of eu_stocks
start(eu_stocks)
end(eu_stocks)
frequency(eu_stocks)

# Generate a simple plot of eu_stocks
plot(eu_stocks)

# Use ts.plot with eu_stocks
ts.plot(eu_stocks, col = 1:4, xlab = "Year", ylab = "Index Value", main = "Major European Stock Indices, 1991-1998")

# Add a legend to your ts.plot
legend("topleft", colnames(eu_stocks), lty = 1, col = 1:4, bty = "n")

```

You've mastered the basics of the `ts` class in R, including encoding `ts` objects, viewing basic time series qualities, and plotting time series data. Now let's move on to the next chapter!

**Trend spotting!**

- some data do not have trends
- linear
  - positive/negative autocorrelation
- rapid growth/decay
- periodic
- variance
- Sample transformations
  - `log()` can linearize a rapid (either values or variance) growth trend 
  - `diff()` can remove a linear trend
  - `diff(..., s)` can remove periodic trends


**Removing trends in variability via the logarithmic transformation**

The logarithmic function `log()` is a data transformation that can be applied to positively valued time series data. It slightly shrinks observations that are greater than one towards zero, while greatly shrinking very large observations. This property can stabilize variability when a series exhibits increasing variability over time. It may also be used to linearize a rapid growth pattern over time.

The time series `rapid_growth` has already been loaded, and is shown in the figure below. Note the vertical range of the data.

![6.png](_images/6.png)
```
# Log rapid_growth
linear_growth <- log(rapid_growth)
  
# Plot linear_growth using ts.plot()
ts.plot(linear_growth)  

```
![7.png](_images/7.png)
As you can see, the logarithmic transformation helps stabilize your data by inducing linear growth over time. Remember to adjust your *interpetation* of the data accordingly.

**Removing trends in level by differencing**

The first difference transformation of a time series $z_t$ consists of the differences (changes) between successive observations over time, that is $z_t−z_{t−1}$.

Differencing a time series can remove a time trend. The function **diff()** will calculate the first difference or change series. A difference series lets you examine the increments or changes in a given time series. It always has one fewer observations than the original series.

The time series `z` has already been loaded, and is shown in the figure below.
![8.png](_images/8.png)
```
# Generate the first difference of z
dz <- diff(z)
  
# Plot dz
ts.plot(dz)  

# View the length of z and dz, respectively
length(z)
######################################################
[1] 200
######################################################

length(dz)
######################################################
[1] 199
######################################################
```
![9.png](_images/9.png)
By removing the long-term time trend, you can view the amount of change from one observation to the next.

**Removing seasonal trends with seasonal differencing**

For time series exhibiting seasonal trends, seasonal differencing can be applied to remove these periodic patterns. For example, monthly data may exhibit a strong twelve month pattern. In such situations, changes in behavior from year to year may be of more interest than changes from month to month, which may largely follow the overall seasonal pattern.

The function `diff(..., lag = s)` will calculate the lag `s` difference or length `s` seasonal change series. For monthly or quarterly data, an appropriate value of `s` would be 12 or 4, respectively. The `diff()` function has `lag = 1` as its default for first differencing. Similar to before, a seasonally differenced series will have `s` fewer observations than the original series.

![10.png](_images/10.png)
```
# Generate a diff of x with n = 4. Save this to dx
dx <- diff(x, lag = 4)
  
# Plot dx
ts.plot(dx)  

# View the length of x and dx, respectively 
length(x)
######################################################
[1] 100
######################################################

length(dx)
######################################################
[1] 96
######################################################
```
![11.png](_images/11.png)
Once again differencing allows you to remove the longer-term time trend - in this case, seasonal volatility - and focus on the change from one period to another.

**The white noise (WN) model**

- White noise
  - the simplest example of a stationary process
- a *weak* white noise process has:
  - a fixed, constant mean
  - a fixed, constant variance
  - no correlation over time



**Simulate the white noise model**

The white noise (WN) model is a basic time series model. It is also a basis for the more elaborate models we will consider. We will focus on the simplest form of WN, independent and identically distributed data.

The [arima.sim()](https://www.rdocumentation.org/packages/stats/versions/3.3.1/topics/arima.sim) function can be used to simulate data from a variety of time series models. ARIMA is an abbreviation for the autoregressive integrated moving average class of models we will consider throughout this course.

An **ARIMA(p, d, q)** model has three parts, the autoregressive order `p`, the order of integration (or differencing) `d`, and the moving average order `q`. We will detail each of these parts soon, but for now we note that the **ARIMA(0, 0, 0)** model, i.e., with all of these components zero, is simply the WN model.

In this exercise, you will practice simulating a basic WN model.
```{r}
# Simulate a WN model with list(order = c(0, 0, 0))
white_noise<- arima.sim(model = list(order = c(0, 0, 0)), n = 100)

# Plot the WN observations
ts.plot(white_noise)

# Simulate from the WN model with: mean = 100, sd = 10
white_noise_2 <- arima.sim(list(order = c(0, 0, 0)), n = 100, mean = 100, sd = 10)

# Plot your white_noise_2 data
ts.plot(white_noise_2)
```

The `arima.sim()` command is a useful way to quickly simulate time series data with the qualities you specify.

**Estimate the white noise model**

For a given time series `y` we can fit the white noise (WN) model using the `arima(..., order = c(0, 0, 0))` function. Recall that the WN model is an ARIMA(0,0,0) model. Applying the **arima()** function returns information or output about the estimated model. For the WN model this includes the estimated mean, labeled `intercept`, and the estimated variance, labeled `sigma^2`.

In this exercise, you'll explore the qualities of the WN model. What is the estimated mean? Compare this with the sample mean using the mean() function. What is the estimated variance? Compare this with the sample variance using the **var()** function.


The time series y has already been loaded, and is shown in the adjoining figure.

![12.png](_images/12.png)
```
# Fit the WN model to y using the arima command
arima(y, order = c(0, 0, 0))

######################################################
Call:
arima(x = y, order = c(0, 0, 0))

Coefficients:
      intercept
        97.5428
s.e.     0.9697

sigma^2 estimated as 94.03:  log likelihood = -369.08,  aic = 742.15
######################################################

# Calculate the sample mean and sample variance of y
mean(y)
######################################################
[1] 97.54284
######################################################

var(y)
######################################################
[1] 94.98175
######################################################
```

From the comparisons you can see that the `arima()` function estimates are very close to the sample mean and variance estimates, in fact identical for the mean.

**The random walk (RW) model**

- Random walk (RW) is a simple example of a non-stationary process
- A random walk has:
  - no specified mean or variance
  - strong dependence over time
  - its changes or increments are white noise (WN)
- The random walk recursion:
  - $Today = Yesterday + Noise$
  - $Y_t = Y_{t-1} + \epsilon_t$
    - where $\epsilon_t$ is mean zero white noise (WN)
  - simulation requires an initial point $Y_0$
  - only one parameter, the WN variance $\sigma^2_\epsilon$
  - as $Y_t - Y_{t-1} = \epsilon_t$ -> `diff(Y)` is WN
- Random walk with a drift
  - $Today = Constant + Yesterday + Noise$
  - $Y_t = c + Y_{t-1} + \epsilon_t$
  - two parameters, the constant $c$, and the WN variance $\sigma^2_\epsilon$
  - $Y_t - Y_{t-1} = ?$ -> WN with mean $c$


**Simulate the random walk model**

The random walk (RW) model is also a basic time series model. It is the cumulative sum (or integration) of a mean zero white noise (WN) series, such that the first difference series of a RW is a WN series. Note for reference that the RW model is an **ARIMA(0, 1, 0)** model, in which the middle entry of 1 indicates that the model's order of integration is 1.

The `arima.sim()` function can be used to simulate data from the RW by including the `model = list(order = c(0, 1, 0))` argument. We also need to specify a series length `n`. Finally, you can specify a `sd` for the series (increments), where the default value is 1.
```{r}
# Generate a RW model using arima.sim
random_walk <- arima.sim(model = list(order = c(0, 1, 0)), n = 100)

# Plot random_walk
ts.plot(random_walk)

# Calculate the first difference series
random_walk_diff <- diff(random_walk)

# Plot random_walk_diff
ts.plot(random_walk_diff)

```

As you can see, the first difference of your `random_walk` data is white noise data. This is because a random walk is simply recursive white noise data. By removing the long-term trend, you end up with simple white noise.

**Simulate the random walk model with a drift**

A random walk (RW) need not wander about zero, it can have an upward or downward trajectory, i.e., a drift or time trend. This is done by including an intercept in the RW model, which corresponds to the slope of the RW time trend.

For an alternative formulation, you can take the cumulative sum of a constant mean white noise (WN) series, such that the mean corresponds to the slope of the RW time trend.

To simulate data from the RW model with a drift you again use the `arima.sim()` function with the `model = list(order = c(0, 1, 0))` argument. This time, you should add the additional argument` mean = ...` to specify the drift variable, or the intercept.
```{r}
# Generate a RW model with a drift uing arima.sim
rw_drift <- arima.sim(model = list(order = c(0, 1, 0)), n = 100, mean = 1)

# Plot rw_drift
ts.plot(rw_drift)

# Calculate the first difference series
rw_drift_diff <- diff(rw_drift)

# Plot rw_drift_diff
ts.plot(rw_drift_diff)

```

Once again, taking the first difference of your random walk data transformed it back into white noise data, regardless of the presence of your long-term drift.

**Estimate the random walk model**

For a given time series `y` we can fit the random walk model with a drift by first differencing the data, then fitting the white noise (WN) model to the differenced data using the `arima()` command with the `order = c(0, 0, 0))` argument.

The `arima()` command displays information or output about the fitted model. Under the `Coefficients:` heading is the estimated drift variable, named the `intercept`. Its approximate standard error (or s.e.) is provided directly below it. The variance of the WN part of the model is also estimated under the label` sigma^2`.

![13.png](_images/13.png)
```{r}
# Difference your random_walk data
rw_diff <- diff(rw_drift)

# Plot rw_diff
ts.plot(rw_diff)

# Now fit the WN model to the differenced data
model_wn <- arima(rw_diff, order = c(0, 0, 0))

# Copy and paste the value of the estimated time trend (intercept) below
int_wn <- model_wn$coef

# Plot the original random_walk data
ts.plot(rw_drift)

# Use abline(0, ...) to add time trend to the figure
abline(0, int_wn)

```
The `arima()` command correctly identified the time trend in your original random-walk data.

**Stationary processes**

- Stationarity
  - stationary models are parsimonious
  - stationary processes have distributional stability over time
- Observed time series:
  - fluctuate randomly
  - but behave similarly from one time period to the next
- Weak stationarity
  - mean, variance, covariance constant over time
  - $Y_1, Y_2, ...$ is a *weakly_stationary* process if:
    - Mean $\mu$ of $Y_t$ is same (constant) for all of $t$
    - Variance $\sigma^2$ of $Y_t$ is same (constant) for all of $t$
    - Covariance of $Y_t$ and $Y_s$ is same (constant) for all $|t - s| = h$ for all $h$
      - e.g., $Cov(Y_3, Y_7) = Cov(Y_7, Y_{11})$
  - estimate $\mu$ accurately by $\bar{y}$
- Stationarity: when?
  - many financial time series do not exhibit stationarity, however:
    - the **changes** in the series are often approximately stationary
    - a stationary series should show random oscillation around some fixed level; a phenomenon called **mean-reversion**


**Are the white noise model or the random walk model stationary?**

The white noise (WN) and random walk (RW) models are very closely related. However, only the RW is always non-stationary, both with and without a drift term. This is a simulation exercise to highlight the differences.

Recall that if we start with a mean zero WN process and compute its running or cumulative sum, the result is a RW process. The [cumsum()](https://www.rdocumentation.org/packages/base/versions/3.3.1/topics/cumsum) function will make this transformation for you. Similarly, if we create a WN process, but change its mean from zero, and then compute its cumulative sum, the result is a RW process with a drift.
```{r}
# Use arima.sim() to generate WN data
white_noise <- arima.sim(model = list(order = c(0, 0, 0)), n = 100)

# Use cumsum() to convert your WN data to RW
random_walk <- cumsum(white_noise)
  
# Use arima.sim() to generate WN drift data
wn_drift <- arima.sim(model = list(order = c(0, 0, 0)), n = 100, mean = 0.4)
  
# Use cumsum() to convert your WN drift data to RW
rw_drift <- cumsum(wn_drift)

# Plot all four data objects
plot.ts(cbind(white_noise, random_walk, wn_drift, rw_drift))

```

As you can see, it is easy to reverse-engineer the RW data by simply generating a cumulative sum of white noise data.

**Scatterplots**


**Asset prices vs. asset returns**

The goal of investing is to make a profit. The revenue or loss from investing depends on the amount invested and changes in prices, and high revenue relative to the size of an investment is of central interest. This is what financial asset returns measure, changes in price as a fraction of the initial price over a given time horizon, for example, one business day.

Let's again consider the `eu_stocks` dataset. This dataset reports index values, which we can regard as prices. The indices are not investable assets themselves, but there are many investable financial assets that closely track major market indices, including mutual funds and exchange traded funds.

Log returns, also called continuously compounded returns, are also commonly used in financial time series analysis. They are the log of gross returns, or equivalently, the changes (or first differences) in the logarithm of prices.

The change in appearance between daily prices and daily returns is typically substantial, while the difference between daily returns and log returns is usually small. As you'll see later, one advantage of using log returns is that calculating multi-period returns from individual periods is greatly simplified - you just add them together!

In this exercise, you'll further explore the `eu_stocks` dataset, including plotting prices, converting prices to (net) returns, and converting prices to log returns.
```{r}
# Plot eu_stocks
plot(eu_stocks)

# Use this code to convert prices to returns
returns <- eu_stocks[-1,] / eu_stocks[-1860,] - 1

# Convert returns to ts
returns <- ts(returns, start = c(1991, 130), frequency = 260)

# Plot returns
plot(returns)

# Use this code to convert prices to log returns
logreturns <- diff(log(eu_stocks))

# Plot logreturns
plot(logreturns)

```

Daily net returns and daily log returns are two valuable metrics for financial data.

**Characteristics of financial time series**

Daily financial asset returns typically share many characteristics. Returns over one day are typically small, and their average is close to zero. At the same time, their variances and standard deviations can be relatively large. Over the course of a few years, several very large returns (in magnitude) are typically observed. These relative outliers happen on only a handful of days, but they account for the most substantial movements in asset prices. Because of these extreme returns, the distribution of daily asset returns is not normal, but heavy-tailed, and sometimes skewed. In general, individual stock returns typically have even greater variability and more extreme observations than index returns.

In this exercise, you'll work with the `eu_percentreturns` dataset, which is the percentage returns calculated from your `eu_stocks` data. For each of the four indices contained in your data, you'll calculate the sample mean, variance, and standard deviation.

Notice that the average daily return is about 0, while the standard deviation is about 1 percentage point. Also apply the hist() and qqnorm() functions to make histograms and normal quantile plots, respectively, for each of the indices.
```{r, message= = FALSE}
eu_percentreturns <- returns * 100

# Generate means from eu_percentreturns
colMeans(eu_percentreturns) 

# Use apply to calculate sample variance from eu_percentreturns
apply(eu_percentreturns, MARGIN = 2, FUN = var)

# Use apply to calculate standard deviation from eu_percentreturns
apply(eu_percentreturns, MARGIN = 2, FUN = sd)

# Display a histogram of percent returns for each index
par(mfrow = c(2,2))
apply(eu_percentreturns, MARGIN = 2, FUN = hist, main = "", xlab = "Percentage Return")

# Display normal quantile plots of percent returns for each index
par(mfrow = c(2,2))
apply(eu_percentreturns, MARGIN = 2, FUN = qqnorm, main = "")
qqline(eu_percentreturns)

```

Note that the vast majority of returns are near zero, but some daily returns are greater than 5 percentage points in magnitude. Similarly, note the clear departure from normality, especially in the tails of the distributions, as evident in the normal quantile plots.

**Plotting pairs of data**

Time series data is often presented in a time series plot. For example, the index values from the `eu_stocks` dataset are shown in the adjoining figure. Recall, `eu_stocks` contains daily closing prices from 1991-1998 for the major stock indices in Germany (`DAX`), Switzerland (`SMI`), France (`CAC`), and the UK (`FTSE`).

It is also useful to examine the bivariate relationship between pairs of time series. In this exercise we will consider the contemporaneous relationship, that is matching observations that occur at the same time, between pairs of index values as well as their log returns. The `plot(a, b)` function will produce a scatterplot when two time series names `a` and `b` are given as input.

To simultaneously make scatterplots for all pairs of several assets the `pairs()` function can be applied to produce a scatterplot matrix. When shared time trends are present in prices or index values it is common to instead compare their returns or log returns.

In this exercise, you'll practice these skills on the `eu_stocks` data. Because the `DAX` and `FTSE` returns have similar time coverage, you can easily make a scatterplot of these indices. Note that the normal distribution has elliptical contours of equal probability, and pairs of data drawn from the multivariate normal distribution form a roughly elliptically shaped point cloud. Do any of the pairs in the scatterplot matrices exhibit this pattern, before or after log transformation?

![14.png](_images/14.png)
```{r}

# Make a scatterplot of DAX and FTSE
plot(eu_stocks[,'DAX'], eu_stocks[,'FTSE'])

# Make a scatterplot matrix of eu_stocks
pairs(eu_stocks)

# Convert eu_stocks to log returns
logreturns <- diff(log(eu_stocks))

# Plot logreturns
plot(logreturns)

# Make a scatterplot matrix of logreturns
pairs(logreturns)

```

As you can see, the `pairs()` command is a useful way to quickly check for relationships between your indices.

**Covariance and correlation**

- Covariance
  - relationship between two variables that is scale dependent
- Correlation
  - standardized version of covariance
  - **+1**: perfectly positive linear relationship
  - **-1**: perfectly negative linear relationship
  - 0 : no linear relationship
  - cov(A, B) / (sd(A)*sd(B))


**Calculating sample covariances and correlations**

Sample covariances measure the strength of the linear relationship between matched pairs of variables. The `cov()` function can be used to calculate covariances for a pair of variables, or a covariance matrix when a matrix containing several variables is given as input. For the latter case, the matrix is symmetric with covariances between variables on the off-diagonal and variances of the variables along the diagonal. On the right you can see the scatterplot matrix of your `logreturns` data.

Covariances are very important throughout finance, but they are not scale free and they can be difficult to directly interpret. Correlation is the standardized version of covariance that ranges in value from -1 to 1, where values close to 1 in magnitude indicate a strong linear relationship between pairs of variables. The `cor()` function can be applied to both pairs of variables as well as a matrix containing several variables, and the output is interpreted analogously.

In this exercise, you'll use `cov()` and `cor()` to explore your `logreturns` data.

![15.png](_images/15.png)
```{r}
DAX_logreturns <- diff(log(eu_stocks[,'DAX']))
FTSE_logreturns <- diff(log(eu_stocks[,'FTSE']))

# Use cov() with DAX_logreturns and FTSE_logreturns
cov(DAX_logreturns, FTSE_logreturns)

# Use cov() with logreturns
cov(logreturns)

# Use cor() with DAX_logreturns and FTSE_logreturns
cor(DAX_logreturns, FTSE_logreturns)

# Use cor() with logreturns
cor(logreturns)

```
The `cov()` and `cor()` commands provide a simple and intuitive output for comparing the relationships between your indices, especially when a scatterplot matrix is difficult to interpret.

**Autocorrelation**


**Calculating autocorrelations**

Autocorrelations or lagged correlations are used to assess whether a time series is dependent on its past. For a time series `x` of length `n` we consider the `n-1` pairs of observations one time unit apart. The first such pair is `(x[2],x[1])`, and the next is `(x[3],x[2])`. Each such pair is of the form `(x[t],x[t-1])` where t is the observation index, which we vary from 2 to n in this case. The lag-1 autocorrelation of `x` can be estimated as the sample correlation of these `(x[t], x[t-1])` pairs.

In general, we can manually create these pairs of observations. First, create two vectors, `x_t0` and `x_t1`, each with length `n-1`, such that the rows correspond to `(x[t], x[t-1])` pairs. Then apply the `cor()` function to estimate the lag-1 autocorrelation.

Luckily, the [acf()](https://www.rdocumentation.org/packages/stats/versions/3.3.1/topics/acf) command provides a shortcut. Applying `acf(..., lag.max = 1, plot = FALSE)` to a series `x` automatically calculates the lag-1 autocorrelation.

Finally, note that the two estimates differ slightly as they use slightly different scalings in their calculation of sample covariance, `1/(n-1)` versus `1/n`. Although the latter would provide a biased estimate, it is preferred in time series analysis, and the resulting autocorrelation estimates only differ by a factor of `(n-1)/n`.

In this exercise, you'll practice both the manual and automatic calculation of a lag-1 autocorrelation. The time series `x` and its length `n` (150) have already been loaded. The series is shown in the plot below.

![16.png](_images/16.png)
```
# Define x_t0 as x[-1]
x_t0 <- x[-1] 

# Define x_t1 as x[-n]
x_t1 <- x[-n]

# Confirm that x_t0 and x_t1 are (x[t], x[t-1]) pairs  
head(cbind(x_t0, x_t1))

######################################################
           x_t0       x_t1
[1,]  1.2996380  2.0655438
[2,]  0.0335780  1.2996380
[3,] -0.3425807  0.0335780
[4,]  0.2325613 -0.3425807
[5,]  0.4681201  0.2325613
[6,]  4.3411156  0.4681201
###################################################### 
  
# Plot x_t0 and x_t1
plot(x_t0, x_t1)

# View the correlation between x_t0 and x_t1
cor(x_t0, x_t1)

######################################################
[1] 0.7630314
######################################################

# Use acf with x
acf(x, lag.max = 1, plot = FALSE)

######################################################
Autocorrelations of series 'x', by lag

    0     1 
1.000 0.758 
######################################################

# Confirm that difference factor is (n-1)/n 
cor(x_t1, x_t0) * (n-1)/n

######################################################
[1] 0.7579445
######################################################

```
![17.png](_images/17.png)

As you can see, the `acf()` command is a helpful shortcut for calculating autocorrelation. In the next few exercises, you'll explore additional features of this command.