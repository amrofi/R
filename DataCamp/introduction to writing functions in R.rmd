---
title: "Introduction to Writing Functions in R"
author: "DataCamp - Richie Cotton"
date: "12/14/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(repos="https://CRAN.R-project.org")
```

## Why you should use functions

**The arguments to mean()**

Mean has 3 arguments

- `x`: a numeric or date-time vector
- `trim`: the proportion of outliers from each end to remove before calculating
- `na.rm`: remove before calculating

**Calling mean()**

Pass arguments by position

`mean(numbers, 0.1, TRUE)`

Pass arguments by name

`mean(na.rm = TRUE, trim = 0.1, x = numbers)`

Common arguments by position, rare arguments by name

`mean(numbers, trim = 0.1, na.rm = TRUE)`

**Benefits of writing functions**

Functions eliminate repetition from your code, which

- can reduce your workload, and
- help avoid errors

Functions also allow code reuse and sharing

**Calling functions**

One way to make your code more readable is to be careful about the order you pass arguments when you call functions, and whether you pass the arguments by position or by name.

`gold_medals`, a numeric vector of the number of gold medals won by each country in the 2016 Summer Olympics, is provided.

For convenience, the arguments of [median()](https://www.rdocumentation.org/packages/stats/topics/median) and [rank()](https://www.rdocumentation.org/packages/base/topics/rank) are displayed using `args()`. Setting `rank()`'s `na.last` argument to `"keep"` means "keep the rank of NA values as NA".

Best practice for calling functions is to include them in the order shown by `args()`, and to only name rare arguments.

```
# Look at the gold medals data
gold_medals
```
```
USA GBR CHN RUS GER JPN FRA KOR ITA AUS NED HUN BRA ESP KEN JAM CRO CUB NZL CAN 
 46  27  26  19  17  12  10   9   8   8   8   8   7   7   6   6   5   5   4   4 
UZB KAZ COL SUI IRI GRE ARG DEN SWE RSA UKR SRB POL PRK BEL THA SVK GEO AZE BLR 
  4   3   3   3   3   3   3   2   2   2   2   2   2   2   2   2   2   2   1   1 
TUR ARM CZE ETH SLO INA ROU BRN VIE TPE BAH IOA CIV FIJ JOR KOS PUR SIN TJK MAS 
  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   0 
MEX VEN ALG IRL LTU BUL IND MGL BDI GRN NIG PHI QAT NOR EGY TUN ISR AUT DOM EST 
  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
FIN MAR NGR POR TTO UAE IOC 
  0   0   0   0   0   0  NA 
```
```
# Note the arguments to median()
args(median)
```
```
function (x, na.rm = FALSE, ...) 
NULL
```
```
# Rewrite this function call, following best practices
median(gold_medals, na.rm = TRUE)
```
```
[1] 1
```

```
# Note the arguments to rank()
args(rank)
```
```
function (x, na.last = TRUE, ties.method = c("average", "first", 
    "last", "random", "max", "min")) 
NULL
```
```
# Rewrite this function call, following best practices
rank(-gold_medals, na.last = "keep", ties.method = "min")
```
```
USA GBR CHN RUS GER JPN FRA KOR ITA AUS NED HUN BRA ESP KEN JAM CRO CUB NZL CAN 
  1   2   3   4   5   6   7   8   9   9   9   9  13  13  15  15  17  17  19  19 
UZB KAZ COL SUI IRI GRE ARG DEN SWE RSA UKR SRB POL PRK BEL THA SVK GEO AZE BLR 
 19  22  22  22  22  22  22  28  28  28  28  28  28  28  28  28  28  28  39  39 
TUR ARM CZE ETH SLO INA ROU BRN VIE TPE BAH IOA CIV FIJ JOR KOS PUR SIN TJK MAS 
 39  39  39  39  39  39  39  39  39  39  39  39  39  39  39  39  39  39  39  60 
MEX VEN ALG IRL LTU BUL IND MGL BDI GRN NIG PHI QAT NOR EGY TUN ISR AUT DOM EST 
 60  60  60  60  60  60  60  60  60  60  60  60  60  60  60  60  60  60  60  60 
FIN MAR NGR POR TTO UAE IOC 
 60  60  60  60  60  60  NA 
```

You've found your calling! Pass the arguments in the order suggested by the documentation, and give names for rarer arguments.

## Converting scripts into functions

![](_images/820.png)

![](_images/821.png)

![](_images/822.png)

![](_images/823.png)

![](_images/824.png)

![](_images/825.png)

![](_images/826.png)

![](_images/827.png)

**Arguments of sample()**

- `x`: a vector of values to sample from
- `size`: how many times do you want to sample from `x`?
- `replace`: should you sample with replacement or not?
- `prob`: a vector of sampling weights for each value of `x`, totalling one.

**Your first function: tossing a coin**

Time to write your first function! It's a really good idea when writing functions to start simple. You can always make a function more complicated later if it's really necessary, so let's not worry about arguments for now.

```{r}
coin_sides <- c("head", "tail")

# Sample from coin_sides once
sample(coin_sides, 1)

# Write a template for your function, toss_coin()
toss_coin <- function() {
  # (Leave the contents of the body for later)
# Add punctuation to finish the body
}

# Paste your script into the function body
toss_coin <- function() {
  coin_sides <- c("head", "tail")
sample(coin_sides, 1)
}

# Call your function
toss_coin()
```

Fantastic function writing! Since there were no arguments to worry about, all you had to do was paste your script inside a function definition template.

**Inputs to functions**

Most functions require some sort of input to determine what to compute. The inputs to functions are called **arguments**. You specify them inside the parentheses after the word "function."

As mentioned in the video, the following exercises assume that you are using [sample()](https://www.rdocumentation.org/packages/base/topics/sample) to do random sampling.

```{r}
coin_sides <- c("head", "tail")
n_flips <- 10

# Sample from coin_sides n_flips times with replacement
sample(coin_sides, n_flips, replace = TRUE)

# Update the function to return n_flips coin tosses
toss_coin <- function(n_flips) {
  coin_sides <- c("head", "tail")
  sample(coin_sides, n_flips, replace = TRUE)
}

# Generate 10 coin tosses
toss_coin(10)
```

Amazing argument addition! The arguments to a function are specified inside the parentheses on the signature line.

**Multiple inputs to functions**

If a function should have more than one argument, list them in the function signature, separated by commas.

To solve this exercise, you need to know how to specify sampling weights to `sample()`. Set the `prob` argument to a numeric vector with the same length as `x`. Each value of `prob` is the probability of sampling the corresponding element of `x`, so their values add up to one. In the following example, each sample has a 20% chance of `"bat"`, a 30% chance of `"cat"` and a 50% chance of `"rat"`.

```
sample(c("bat", "cat", "rat"), 10, replace = TRUE, prob = c(0.2, 0.3, 0.5))
```

```{r}
coin_sides <- c("head", "tail")
n_flips <- 10
p_head <- 0.8

# Define a vector of weights
weights <- c(p_head, 1 - p_head)

# Update so that heads are sampled with prob p_head
sample(coin_sides, n_flips, replace = TRUE, prob = weights)

# Update the function so heads have probability p_head
toss_coin <- function(n_flips, p_head) {
  coin_sides <- c("head", "tail")
  # Define a vector of weights
  weights <- c(p_head, 1 - p_head)
  # Modify the sampling to be weighted
  sample(coin_sides, n_flips, replace = TRUE, prob = weights)
}

# Generate 10 coin tosses
toss_coin(10, .80)
```

Magnificent multiple-argument specification! When you have multiple arguments in the function signature, their names are separated by commas.

## Y kant I reed ur code?

**dplyr verbs()**

- `select()` *selects* columns
- `filter()` *filters* rows

**Function names should contain a verb**

- get
- calculate (or maybe just calc)
- run
- process
- import
- clean
- tidy
- draw

**lm() is badly named**

- acronymns aren't self-explanatory
- it doesn't contain a verb
- there are lots of different linear models

A better name would be `run_linear_regression()`

**Readability vs. typeability**
- understanding code >> typing code
- code editors have autocomplete
- you can alias common functions

![](_images/828.png)

**Arguments of lm()**
```{r}
args(lm)
```

![](_images/829.png)

![](_images/830.png)

**Renaming GLM**

R's generalized linear regression function, `glm()`, suffers the same usability problems as `lm()`: its name is an acronym, and its `formula` and `data` arguments are in the wrong order.

To solve this exercise, you need to know two things about generalized linear regression:

1. `glm()` formulas are specified like `lm()` formulas: response is on the left, and explanatory variables are added on the right.
2. To model count data, set `glm()`'s `family` argument to `poisson`, making it a Poisson regression.

Here you'll use data on the [number of yearly visits to Snake River](https://www.rdocumentation.org/packages/COUNT/topics/loomis) at Jackson Hole, Wyoming, `snake_river_visits`.

```{r}
library(dplyr)
snake_river_visits <- readRDS("_data/snake_river_visits.rds")

# Run a generalized linear regression 
glm(
  # Model no. of visits vs. gender, income, travel
  n_visits ~ gender + income + travel, 
  # Use the snake_river_visits dataset
  data = snake_river_visits, 
  # Make it a Poisson regression
  family = poisson
)

# Write a function to run a Poisson regression
run_poisson_regression <- function(data, formula) {
    glm(formula, data, family = poisson)
}

# Re-run the Poisson regression, using your function
model <- snake_river_visits %>%
  run_poisson_regression(n_visits ~ gender + income + travel)
```
```
# Run this to see the predictions
snake_river_explanatory %>%
  mutate(predicted_n_visits = predict(model, ., type = "response"))%>%
  arrange(desc(predicted_n_visits))
```
```
   gender      income     travel predicted_n_visits
1  female   [$0,$25k] [0h,0.25h]          86.518598
2  female ($25k,$55k] [0h,0.25h]          84.813684
3    male   [$0,$25k] [0h,0.25h]          59.524843
4    male ($25k,$55k] [0h,0.25h]          58.351861
5  female ($95k,$Inf) [0h,0.25h]          48.526883
6  female ($55k,$95k] [0h,0.25h]          48.408009
7  female   [$0,$25k] (0.25h,4h]          46.212343
8  female ($25k,$55k] (0.25h,4h]          45.301694
9    male ($95k,$Inf) [0h,0.25h]          33.386522
10   male ($55k,$95k] [0h,0.25h]          33.304737
11   male   [$0,$25k] (0.25h,4h]          31.794117
12   male ($25k,$55k] (0.25h,4h]          31.167590
13 female ($95k,$Inf) (0.25h,4h]          25.919756
14 female ($55k,$95k] (0.25h,4h]          25.856261
15   male ($95k,$Inf) (0.25h,4h]          17.832806
16   male ($55k,$95k] (0.25h,4h]          17.789122
17 female   [$0,$25k]  (4h,Infh)           7.670599
18 female ($25k,$55k]  (4h,Infh)           7.519444
19   male   [$0,$25k]  (4h,Infh)           5.277376
20   male ($25k,$55k]  (4h,Infh)           5.173382
21 female ($95k,$Inf)  (4h,Infh)           4.302315
22 female ($55k,$95k]  (4h,Infh)           4.291776
23   male ($95k,$Inf)  (4h,Infh)           2.959995
24   male ($55k,$95k]  (4h,Infh)           2.952744
```

Neat naming! Low-income females who live within 15 mins travel are predicted to visit Snake River over 80 times in a year!

## Default arguments

![](_images/831.png)

![](_images/832.png)

![](_images/833.png)

![](_images/834.png)

![](_images/835.png)

![](_images/836.png)

![](_images/837.png)

**Numeric defaults**

`cut_by_quantile()` converts a numeric vector into a categorical variable where quantiles define the cut points. This is a useful function, but at the moment you have to specify five arguments to make it work. This is too much thinking and typing.

By specifying default arguments, you can make it easier to use. Let's start with `n`, which specifies how many categories to cut `x` into.

A numeric vector of the number of visits to Snake River is provided as `n_visits`.

```{r}
n_visits <- snake_river_visits$n_visits

# Set the default for n to 5
cut_by_quantile <- function(x, n = 5, na.rm, labels, interval_type) {
  probs <- seq(0, 1, length.out = n + 1)
  qtiles <- quantile(x, probs, na.rm = na.rm, names = FALSE)
  right <- switch(interval_type, "(lo, hi]" = TRUE, "[lo, hi)" = FALSE)
  cut(x, qtiles, labels = labels, right = right, include.lowest = TRUE)
}

# Remove the n argument from the call
cut_by_quantile(
  n_visits,
  na.rm = FALSE, 
  labels = c("very low", "low", "medium", "high", "very high"),
  interval_type = "(lo, hi]"
)
```

Nice numeric default setting! Remember to only set defaults for numeric *detail* arguments, not *data* arguments.

**Logical defaults**

`cut_by_quantile()` is now slightly easier to use, but you still always have to specify the `na.rm` argument. This removes missing values – it behaves the same as the `na.rm` argument to `mean()` or `sd()`.

Where functions have an argument for removing missing values, the best practice is to not remove them by default (in case you hadn't spotted that you had missing values). That means that the default for `na.rm` should be `FALSE`.

```{r}
# Set the default for na.rm to FALSE
cut_by_quantile <- function(x, n = 5, na.rm = FALSE, labels, interval_type) {
  probs <- seq(0, 1, length.out = n + 1)
  qtiles <- quantile(x, probs, na.rm = na.rm, names = FALSE)
  right <- switch(interval_type, "(lo, hi]" = TRUE, "[lo, hi)" = FALSE)
  cut(x, qtiles, labels = labels, right = right, include.lowest = TRUE)
}

# Remove the na.rm argument from the call
cut_by_quantile(
  n_visits,
  labels = c("very low", "low", "medium", "high", "very high"),
  interval_type = "(lo, hi]"
)
```

Lovely logical default setting! For most variable types, you can just set a default using `arg = default_value` in the function signature.

**NULL defaults**

The `cut()` function used by `cut_by_quantile()` can automatically provide sensible labels for each category. The code to generate these labels is [pretty complicated](https://github.com/wch/r-source/blob/29a9e663a2352843a6ea26b259725b0b97d0e4bd/src/library/base/R/cut.R#L42-L60), so rather than appearing in the function signature directly, its `labels` argument defaults to `NULL`, and the calculation details are shown on the [?cut](https://www.rdocumentation.org/packages/base/topics/cut) help page.

```{r}
# Set the default for labels to NULL
cut_by_quantile <- function(x, n = 5, na.rm = FALSE, labels = NULL, interval_type) {
  probs <- seq(0, 1, length.out = n + 1)
  qtiles <- quantile(x, probs, na.rm = na.rm, names = FALSE)
  right <- switch(interval_type, "(lo, hi]" = TRUE, "[lo, hi)" = FALSE)
  cut(x, qtiles, labels = labels, right = right, include.lowest = TRUE)
}

# Remove the labels argument from the call
cut_by_quantile(
  n_visits,
  interval_type = "(lo, hi]"
)
```

Neat `NULL` default setting! If you use this capability, make sure to document how the argument behaves in the function's help page.

**Categorical defaults**

When cutting up a numeric vector, you need to worry about what happens if a value lands exactly on a boundary. You can either put this value into a category of the lower interval or the higher interval. That is, you can choose your intervals to include values at the top boundary but not the bottom (in mathematical terminology, "open on the left, closed on the right", or `(lo, hi]`). Or you can choose the opposite ("closed on the left, open on the right", or `[lo, hi)`). `cut_by_quantile()` should allow these two choices.

The pattern for categorical defaults is:

```
function(cat_arg = c("choice1", "choice2")) {
  cat_arg <- match.arg(cat_arg)
}
```

**Free hint:** In the console, type `head(rank)` to see the start of `rank()`'s definition, and look at the `ties.method` argument.

```{r}
# Set the categories for interval_type to "(lo, hi]" and "[lo, hi)"
cut_by_quantile <- function(x, n = 5, na.rm = FALSE, labels = NULL, 
                            interval_type = c("(lo, hi]", "[lo, hi)")) {
  # Match the interval_type argument
  interval_type <- match.arg(interval_type)
  probs <- seq(0, 1, length.out = n + 1)
  qtiles <- quantile(x, probs, na.rm = na.rm, names = FALSE)
  right <- switch(interval_type, "(lo, hi]" = TRUE, "[lo, hi)" = FALSE)
  cut(x, qtiles, labels = labels, right = right, include.lowest = TRUE)
}

# Remove the interval_type argument from the call
cut_by_quantile(n_visits)
```

Clever categorical default setting! As a bonus, `match.arg()` handles throwing an error if the user types a value that wasn't specified.

## Passing arguments between functions

![](_images/838.png)

![](_images/839.png)

![](_images/840.png)

![](_images/841.png)

![](_images/842.png)

**Harmonic mean**

The harmonic mean is the reciprocal of the arithmetic mean of the reciprocal of the data. That is

$harmonic\ mean(x) = 1/arithmetic\ mean(1/x)$

The harmonic mean is often used to average ratio data. You'll be using it on the price/earnings ratio of stocks in the Standard and Poor's 500 index, provided as `std_and_poor500`. Price/earnings ratio is a measure of how expensive a stock is.

The `dplyr` package is loaded.

```{r}
std_and_poor500 <- readRDS("_data/std_and_poor500_with_pe_2019-06-21.rds")

# Look at the Standard and Poor 500 data
glimpse(std_and_poor500)

# Write a function to calculate the reciprocal
get_reciprocal <- function(x) {
  1/x
}

# Write a function to calculate the harmonic mean
calc_harmonic_mean <- function(x) {
  x %>%
    get_reciprocal() %>%
    mean() %>%
    get_reciprocal()
}

std_and_poor500 %>% 
  # Group by sector
  group_by(sector) %>% 
  # Summarize, calculating harmonic mean of P/E ratio
  summarize(hmean_pe_ratio = calc_harmonic_mean(pe_ratio))
```

Happy harmonizing! It looks like we have a problem though: most sectors have missing values.

**Dealing with missing values**

In the last exercise, many sectors had an `NA` value for the harmonic mean. It would be useful for your function to be able to remove missing values before calculating.

Rather than writing your own code for this, you can outsource this functionality to `mean()`.

The `dplyr` package is loaded.

```{r}
# Add an na.rm arg with a default, and pass it to mean()
calc_harmonic_mean <- function(x, na.rm = FALSE) {
  x %>%
    get_reciprocal() %>%
    mean(na.rm = na.rm) %>%
    get_reciprocal()
}

std_and_poor500 %>% 
  # Group by sector
  group_by(sector) %>% 
  # Summarize, calculating harmonic mean of P/E ratio
  summarize(hmean_pe_ratio = calc_harmonic_mean(pe_ratio, na.rm = TRUE))
```

Marvelous missing value removal! Using this metric, Real Estate is by far the most expensive sector.

**Passing arguments with ...**

Rather than explicitly giving `calc_harmonic_mean()` and `na.rm` argument, you can use `...` to simply "pass other arguments" to `mean()`.

The `dplyr` package is loaded.

```{r}
# Swap na.rm arg for ... in signature and body
calc_harmonic_mean <- function(x, ...) {
  x %>%
    get_reciprocal() %>%
    mean(...) %>%
    get_reciprocal()
}

std_and_poor500 %>% 
  # Group by sector
  group_by(sector) %>% 
  # Summarize, calculating harmonic mean of P/E ratio
  summarize(hmean_pe_ratio = calc_harmonic_mean(pe_ratio, na.rm = TRUE))
```

Delightful use of dots! Did you notice that this code was the same as in the previous exercise? Using `...` doesn't change how people use your function; it just means the function is more flexible. Whether flexible means better (or not) is up to you to decide.

## Checking arguments

![](_images/843.png)

![](_images/844.png)

![](_images/845.png)

![](_images/846.png)

![](_images/847.png)

![](_images/848.png)

![](_images/849.png)

![](_images/850.png)

![](_images/851.png)

**Throwing errors with bad arguments**

If a user provides a bad input to a function, the best course of action is to throw an error letting them know. The two rules are

1. Throw the error message as soon as you realize there is a problem (typically at the start of the function).
2. Make the error message easily understandable.

You can use the `assert_*()` functions from `assertive` to check inputs and throw errors when they fail.

```{r error=TRUE}
library(assertive)

calc_harmonic_mean <- function(x, na.rm = FALSE) {
  # Assert that x is numeric
  assert_is_numeric(x)
  x %>%
    get_reciprocal() %>%
    mean(na.rm = na.rm) %>%
    get_reciprocal()
}

# See what happens when you pass it strings
calc_harmonic_mean(std_and_poor500$sector)
```

Amazing assertions! Providing human-readable error messages is important when your users are humans!

**Custom error logic**

Sometimes the `assert_*()` functions in `assertive` don't give the most informative error message. For example, the assertions that check if a number is in a numeric range will tell the user that a value is out of range, but then won't say why that's a problem. In that case, you can use the `is_*()` functions in conjunction with messages, warnings, or errors to define custom feedback.

The harmonic mean only makes sense when `x` has all positive values. (Try calculating the harmonic mean of one and minus one to see why.) Make sure your users know this!

```{r error=TRUE}
calc_harmonic_mean <- function(x, na.rm = FALSE) {
  assert_is_numeric(x)
  # Check if any values of x are non-positive
  if(any(is_non_positive(x), na.rm = TRUE)) {
    # Throw an error
    stop("x contains non-positive values, so the harmonic mean makes no sense.")
  }
  x %>%
    get_reciprocal() %>%
    mean(na.rm = na.rm) %>%
    get_reciprocal()
}

# See what happens when you pass it negative numbers
calc_harmonic_mean(std_and_poor500$pe_ratio - 20)
```

Cool custom logic! Explaining what went wrong is helpful to users. Explaining why it is wrong is even better!

**Fixing function arguments**

The harmonic mean function is almost complete. However, you still need to provide some checks on the `na.rm` argument. This time, rather than throwing errors when the input is in an incorrect form, you are going to try to fix it.

`na.rm` should be a logical vector with one element (that is, `TRUE`, or `FALSE`).

The `assertive` package is loaded for you.

```{r}
# Update the function definition to fix the na.rm argument
calc_harmonic_mean <- function(x, na.rm = FALSE) {
  assert_is_numeric(x)
  if(any(is_non_positive(x), na.rm = TRUE)) {
    stop("x contains non-positive values, so the harmonic mean makes no sense.")
  }
  # Use the first value of na.rm, and coerce to logical
  na.rm <- coerce_to(use_first(na.rm), target_class = "logical")
  x %>%
    get_reciprocal() %>%
    mean(na.rm = na.rm) %>%
    get_reciprocal()
}

# See what happens when you pass it malformed na.rm
calc_harmonic_mean(std_and_poor500$pe_ratio, na.rm = 1:5)
```

Considerate argument correction! For small problems with inputs, it can be better to fix things rather than throwing an error.

## Returning values from functions

**Reasons for returning early**

1. you already know the answer
2. the input is an edge case

![](_images/852.png)

![](_images/853.png)

**Returning early**

Sometimes, you don't need to run through the whole body of a function to get the answer. In that case you can return early from that function using `return()`.

To check if `x` is divisible by `n`, you can use `is_divisible_by(x, n)` from `assertive`.

Alternatively, use the modulo operator, `%%`. `x %% n` gives the remainder when dividing `x` by `n`, so `x %% n == 0` determines whether `x` is divisible by `n`. Try `1:10 %% 3 == 0` in the console.

To solve this exercise, you need to know that a leap year is every 400th year (like the year 2000) or every 4th year that isn't a century (like 1904 but not 1900 or 1905).

`assertive` is loaded.

```{r}
is_leap_year <- function(year) {
  # If year is div. by 400 return TRUE
  if(is_divisible_by(year, 400)) {
    return(TRUE)
  }
  # If year is div. by 100 return FALSE
  if(is_divisible_by(year, 100)) {
    return(FALSE)
  }  
  # If year is div. by 4 return TRUE
  if(is_divisible_by(year, 4)) {
    return(TRUE)
  }
  # Otherwise return FALSE
  FALSE
}
```

Yay for leap year identification! Returning early can often improve the performance of your functions considerably for some input values.

**Returning invisibly**

When the main purpose of a function is to generate output, like drawing a plot or printing something in the console, you may not want a return value to be printed as well. In that case, the value should be [invisibly returned](https://www.rdocumentation.org/packages/base/topics/invisible).

The base R plot function returns `NULL`, since its main purpose is to draw a plot. This isn't helpful if you want to use it in piped code: instead it should invisibly return the plot data to be piped on to the next step.

Recall that `plot()` has a [formula interface](https://www.rdocumentation.org/packages/graphics/topics/plot.formula): instead of giving it vectors for `x` and `y`, you can specify a formula describing which columns of a data frame go on the `x` and `y` axes, and a data argument for the data frame. Note that just like `lm()`, the arguments are the wrong way round because the detail argument, `formula`, comes before the data argument.

```
plot(y ~ x, data = data)
```

```{r}
# Using cars, draw a scatter plot of dist vs. speed
plt_dist_vs_speed <- plot(dist ~ speed, data = cars)

# Oh no! The plot object is NULL
plt_dist_vs_speed

# Define a pipeable plot fn with data and formula args
pipeable_plot <- function(data, formula) {
  # Call plot() with the formula interface
  plot(formula, data)
  # Invisibly return the input dataset
  invisible(data)
}

# Draw the scatter plot of dist vs. speed again
plt_dist_vs_speed <- cars %>% 
  pipeable_plot(dist ~ speed)

# Now the plot object has a value
plt_dist_vs_speed
```

Premium pipeable plotting! Return values are usually desirable (so you can use the objects in later code), even if you don't want them printing to the console.

## Returning multiple values from functions

![](_images/854.png)

![](_images/855.png)

![](_images/856.png)

![](_images/857.png)

![](_images/858.png)

![](_images/859.png)

![](_images/860.png)

![](_images/861.png)

**When to use each technique**

- if you need the result to have a particular type, add additional return values as attributes
- otherwise, collect all return values into a list

![](_images/862.png)

**Returning many things**

Functions can only return one value. If you want to return multiple things, then you can store them all in a list.

If users want to have the list items as separate variables, they can assign each list element to its own variable using `zeallot`'s multi-assignment operator, `%<-%`.

`glance()`, `tidy()`, and `augment()` each take the model object as their only argument.

The Poisson regression model of Snake River visits is available as `model`. `broom` and `zeallot` are loaded.

```{r}
library(broom)
library(zeallot)

# Look at the structure of model (it's a mess!)
str(model)

# Use broom tools to get a list of 3 data frames
list(
  # Get model-level values
  model = glance(model),
  # Get coefficient-level values
  coefficients = tidy(model),
  # Get observation-level values
  observations = augment(model)
) 

# Wrap this code into a function, groom_model
groom_model <- function(model){
  list(
    model = glance(model),
    coefficients = tidy(model),
    observations = augment(model)
  )
}

# Call groom_model on model, assigning to 3 variables
c(mdl, cff, obs) %<-% groom_model(model)

# See these individual variables
mdl; cff; obs
```

Magnificent multi-assignment! Returning many values is as easy as collecting them into a list. The groomed model has data frames that are easy to program against.

**Returning metadata**

Sometimes you want the return multiple things from a function, but you want the result to have a particular class (for example, a data frame or a numeric vector), so returning a list isn't appropriate. This is common when you have a result plus metadata about the result. (Metadata is "data about the data". For example, it could be the file a dataset was loaded from, or the username of the person who created the variable, or the number of iterations for an algorithm to converge.)

In that case, you can store the metadata in attributes. Recall the syntax for assigning attributes is as follows.

```
attr(object, "attribute_name") <- attribute_value
```

```{r}
pipeable_plot <- function(data, formula) {
  plot(formula, data)
  # Add a "formula" attribute to data
  attr(data, "formula") <- formula
  invisible(data)
}

# From previous exercise
plt_dist_vs_speed <- cars %>% 
  pipeable_plot(dist ~ speed)

# Examine the structure of the result
str(plt_dist_vs_speed)
```

Astounding attribute setting! You can include metadata in the return value by storing it as attributes.

## Environments

![](_images/863.png)

![](_images/864.png)

![](_images/865.png)

![](_images/866.png)

**Creating and exploring environments**

Environments are used to store other variables. Mostly, you can think of them as lists, but there's an important extra property that is relevant to writing functions. Every environment has a **parent environment** (except the **empty environment**, at the root of the environment tree). This determines which variables R know about at different places in your code.

Facts about the Republic of South Africa are contained in `capitals`, `national_parks`, and `population`.

```
# Add capitals, national_parks, & population to a named list
rsa_lst <- list(
  capitals = capitals,
  national_parks = national_parks,
  population = population
)

# List the structure of each element of rsa_lst
ls.str(rsa_lst)
```
```
List of 3
 $ capitals      :Classes 'tbl_df', 'tbl' and 'data.frame':	3 obs. of  2 variables:
  ..$ city           : chr [1:3] "Cape Town" "Bloemfontein" "Pretoria"
  ..$ type_of_capital: chr [1:3] "Legislative" "Judicial" "Administrative"
 $ national_parks: chr [1:22] "Addo Elephant National Park" "Agulhas National Park" "Ai-Ais/Richtersveld Transfrontier Park" "Augrabies Falls National Park" ...
 $ population    : Time-Series [1:5] from 1996 to 2016: 40583573 44819778 47390900 51770560 55908900
```
```
# Convert the list to an environment
rsa_env <- list2env(rsa_lst)

# List the structure of each variable
ls.str(rsa_env)
```
```
<environment: 0x5573c5509448>
```
```
# Find the parent environment of rsa_env
parent <- parent.env(rsa_env)

# Print its name
environmentName(parent)
```
```
[1] "R_GlobalEnv"
```

Excellent environment exploration! The parent of the environment you defined is the global environment. Fun fact: South Africa has more capital cities than any other country. Did you enjoy working with “big data”?

**Do variables exist?**

If R cannot find a variable in the current environment, it will look in the parent environment, then the grandparent environment, and so on until it finds it.

`rsa_env` has been modified so it includes `capitals` and `national_parks`, but not `population`.

```
# Compare the contents of the global environment and rsa_env
ls.str(globalenv())
```
```
capitals : Classes 'tbl_df', 'tbl' and 'data.frame':	3 obs. of  2 variables:
 $ city           : chr  "Cape Town" "Bloemfontein" "Pretoria"
 $ type_of_capital: chr  "Legislative" "Judicial" "Administrative"
data_dir :  chr "/usr/local/share/datasets"
national_parks :  chr [1:22] "Addo Elephant National Park" "Agulhas National Park" ...
population :  Time-Series [1:5] from 1996 to 2016: 40583573 44819778 47390900 51770560 55908900
rsa_env : <environment: 0x5555651bf3b8> 
```
```
ls.str(rsa_env)
```
```
capitals : Classes 'tbl_df', 'tbl' and 'data.frame':	3 obs. of  2 variables:
 $ city           : chr  "Cape Town" "Bloemfontein" "Pretoria"
 $ type_of_capital: chr  "Legislative" "Judicial" "Administrative"
national_parks :  chr [1:22] "Addo Elephant National Park" "Agulhas National Park" ...
```
```
# Does population exist in rsa_env?
exists("population", envir = rsa_env)
```
```
[1] TRUE
```
```
# Does population exist in rsa_env, ignoring inheritance?
exists("population", envir = rsa_env, inherits = FALSE)
```
```
[1] FALSE
```

Elegant existence checking! R searches for variables in all the parent environments, unless you explicitly tell it not to.

## Scope and precedence

![](_images/867.png)

## Grain yields and unit conversion

![](_images/868.png)

![](_images/869.png)

![](_images/870.png)

![](_images/871.png)

![](_images/872.png)

![](_images/873.png)

**Converting areas to metric 1**

In this chapter, you'll be working with [grain yield data](https://www.rdocumentation.org/packages/agridat/topics/nass.corn) from the[ United States Department of Agriculture, National Agricultural Statistics Service](http://quickstats.nass.usda.gov/). Unfortunately, they report all areas in acres. So, the first thing you need to do is write some utility functions to convert areas in acres to areas in hectares.

To solve this exercise, you need to know the following:

1. There are 4840 square yards in an acre.
2. There are 36 inches in a yard and one inch is 0.0254 meters.
3. There are 10000 square meters in a hectare.

```{r}
# Write a function to convert acres to sq. yards
acres_to_sq_yards <- function(acres) {
  acres * 4840
}

# Write a function to convert yards to meters
yards_to_meters <- function(yards) {
  yards*36*0.0254
}

# Write a function to convert sq. meters to hectares
sq_meters_to_hectares <- function(sq_meters) {
  sq_meters/10000
}
```

Delightful distance and area conversion! Even such simple one-line functions as these can be very useful.

**Converting areas to metric 2**

You're almost there with creating a function to convert acres to hectares. You need another utility function to deal with getting from square yards to square meters. Then, you can bring everything together to write the overall acres-to-hectares conversion function. Finally, in the next exercise you'll be calculating area conversions in the denominator of a ratio, so you'll need a harmonic acre-to-hectare conversion function.

Free hints: magrittr's `raise_to_power()` will be useful here. The last step is similar to Chapter 2's [Harmonic Mean](https://campus.datacamp.com/courses/introduction-to-function-writing-in-r/all-about-arguments?ex=7).

The three utility functions from the last exercise (`acres_to_sq_yards()`, `yards_to_meters()`, and `sq_meters_to_hectares()`) are available, as is your `get_reciprocal()` from Chapter 2. `magrittr` is loaded.

```{r}
library(magrittr)

# Write a function to convert sq. yards to sq. meters
sq_yards_to_sq_meters <- function(sq_yards) {
  sq_yards %>%
    # Take the square root
    sqrt() %>%
    # Convert yards to meters
    yards_to_meters() %>%
    # Square it
    raise_to_power(2)
}

# Write a function to convert acres to hectares
acres_to_hectares <- function(acres) {
  acres %>%
    # Convert acres to sq yards
    acres_to_sq_yards() %>%
    # Convert sq yards to sq meters
    sq_yards_to_sq_meters() %>%
    # Convert sq meters to hectares
    sq_meters_to_hectares()
}

# Define a harmonic acres to hectares function
harmonic_acres_to_hectares <- function(acres) {
  acres %>% 
    # Get the reciprocal
    get_reciprocal() %>%
    # Convert acres to hectares
    acres_to_hectares() %>% 
    # Get the reciprocal again
    get_reciprocal()
}
```

Amazing area conversion! By breaking down this conversion into lots of simple functions, you have easy to read code, which helps guard against bugs.

**Converting yields to metric**

The yields in the NASS corn data are also given in US units, namely bushels per acre. You'll need to write some more utility functions to convert this unit to the metric unit of kg per hectare.

Bushels historically meant a volume of 8 gallons, but in the context of grain, they are now defined as masses. This mass differs for each grain! To solve this exercise, you need to know these facts.

1. One pound (lb) is 0.45359237 kilograms (kg).
2. One bushel is 48 lbs of barley, 56 lbs of corn, or 60 lbs of wheat.

`magrittr` is loaded.

```{r}
# Write a function to convert lb to kg
lbs_to_kgs <- function(lbs) {lbs * 0.45359237}

# Write a function to convert bushels to lbs
bushels_to_lbs <- function(bushels, crop) {
  # Define a lookup table of scale factors
  c(barley = 48, corn = 56, wheat = 60) %>%
    # Extract the value for the crop
    extract(crop) %>%
    # Multiply by the no. of bushels
    multiply_by(bushels)
}

# Write a function to convert bushels to kg
bushels_to_kgs <- function(bushels, crop) {
  bushels %>%
    # Convert bushels to lbs for this crop
    bushels_to_lbs(crop) %>%
    # Convert lbs to kgs
    lbs_to_kgs()
}

# Write a function to convert bushels/acre to kg/ha
bushels_per_acre_to_kgs_per_hectare <- function(bushels_per_acre, crop = c("barley", "corn", "wheat")) {
  # Match the crop argument
  crop <- match.arg(crop)
  bushels_per_acre %>%
    # Convert bushels to kgs for this crop
    bushels_to_kgs(crop) %>%
    # Convert harmonic acres to ha
    harmonic_acres_to_hectares()
}
```

Courageous conversion! You now have a full stack of functions for converting the units. Time to try them out!

**Applying the unit conversion**

Now that you've written some functions, it's time to apply them! The NASS `corn` dataset is available, and you can fortify it (jargon for "adding new columns") with metrics areas and yields.

This fortification process can also be turned in to a function, so you'll define a function for this, and test it on the NASS `wheat` dataset.

```{r}
corn <- readRDS("_data/nass.corn.rds")
wheat <- readRDS("_data/nass.wheat.rds")

# View the corn dataset
glimpse(corn)

corn %>%
  # Add some columns
  mutate(
    # Convert farmed area from acres to ha
    farmed_area_ha = acres_to_hectares(farmed_area_acres),
    # Convert yield from bushels/acre to kg/ha
    yield_kg_per_ha = bushels_per_acre_to_kgs_per_hectare(
      yield_bushels_per_acre, 
      crop = "corn"
    )
  )

# Wrap this code into a function
fortify_with_metric_units <- function(data, crop) {
  data %>%
    mutate(
      farmed_area_ha = acres_to_hectares(farmed_area_acres),
      yield_kg_per_ha = bushels_per_acre_to_kgs_per_hectare(
        yield_bushels_per_acre, 
        crop = crop
      )
    )
}

# Try it on the wheat dataset
fortify_with_metric_units(wheat, crop = "wheat")
```

Meticulous metric action! Now that everything is wrapped into functions, notice how easy it is to apply the update to new datasets: just one line of code!

## Visualizing grain yields

![](_images/874.png)

![](_images/875.png)

![](_images/876.png)

![](_images/877.png)

**Plotting yields over time**

Now that the units have been dealt with, it's time to explore the datasets. An obvious question to ask about each crop is, "how do the yields change over time in each US state?" Let's draw a line plot to find out!

`ggplot2` is loaded, and `corn` and `wheat` datasets are available with metric units.

```{r}
library(ggplot2)
corn <- fortify_with_metric_units(corn, crop = "corn")
wheat <- fortify_with_metric_units(wheat, crop = "wheat")

# Using corn, plot yield (kg/ha) vs. year
ggplot(corn, aes(year, yield_kg_per_ha)) +
  # Add a line layer, grouped by state
  geom_line(aes(group = state)) +
  # Add a smooth trend layer
  geom_smooth()

# Wrap this plotting code into a function
plot_yield_vs_year <- function(data) {
  ggplot(data, aes(year, yield_kg_per_ha)) +
    geom_line(aes(group = state)) +
    geom_smooth()
}

# Test it on the wheat dataset
plot_yield_vs_year(wheat)
```

Perspicacious plotting! Look at the huge increase in yields from the time of the Green Revolution in the 1950s.

**A nation divided**

The USA has a varied climate, so we might expect yields to differ between states. Rather than trying to reason about 50 states separately, we can use the USA Census Regions to get 9 groups.

The "Corn Belt", where most US corn is grown is in the "West North Central" and "East North Central" regions. The "Wheat Belt" is in the "West South Central" region.

`dplyr` is loaded, the `corn` and `wheat` datasets are available, as is `usa_census_regions`.

```{r}
census_regions <- c("New England", "New England", "New England", "New England", "New England", "New England", "Mid-Atlantic", "Mid-Atlantic", "Mid-Atlantic", "East North Central", "East North Central", "East North Central", "East North Central", "East North Central", "West North Central", "West North Central", "West North Central", "West North Central", "West North Central", "West North Central", "West North Central", "South Atlantic", "South Atlantic", "South Atlantic", "South Atlantic", "South Atlantic", "South Atlantic", "South Atlantic", "South Atlantic", "South Atlantic", "East South Central", "East South Central", "East South Central", "East South Central", "West South Central", "West South Central", "West South Central", "West South Central", "Mountain", "Mountain", "Mountain", "Mountain", "Mountain", "Mountain", "Mountain", "Mountain", "Pacific", "Pacific", "Pacific", "Pacific", "Pacific")

state <- c("Connecticut", "Maine", "Massachusetts", "New Hampshire", "Rhode Island", "Vermont", "New Jersey", "New York", "Pennsylvania", "Illinois", "Indiana", "Michigan", "Ohio", "Wisconsin", "Iowa", "Kansas", "Minnesota", "Missouri",    "Nebraska", "North Dakota", "South Dakota", "Delaware", "Florida", "Georgia", "Maryland", "North Carolina", "South Carolina", "Virginia", "District of Columbia","West Virginia","Alabama", "Kentucky", "Mississippi", "Tennessee", "Arkansas", "Louisiana", "Oklahoma", "Texas", "Arizona", "Colorado", "Idaho", "Montana", "Nevada", "New Mexico", "Utah", "Wyoming", "Alaska", "California", "Hawaii", "Oregon", "Washington")

usa_census_regions <- tibble (census_regions, state)

# Inner join the corn dataset to usa_census_regions by state
corn %>%
  inner_join(usa_census_regions, by = "state")

# Wrap this code into a function
fortify_with_census_region <- function(data) {
  data %>%
    inner_join(usa_census_regions, by = "state")
}

# Try it on the wheat dataset
fortify_with_census_region(wheat)
```

Judicious joining! With the census data incorporated into the crop datasets, you can now look at yield differences between the regions.

**Plotting yields over time by region**

So far, you have a function to plot yields over time for each crop, and you've added a `census_region` column to the crop datasets. Now you are ready to look at how the yields change over time in each region of the USA.

`ggplot2` is loaded. `corn` and `wheat` have been fortified with census regions. `plot_yield_vs_year()` is available.

```{r error=TRUE}
corn <- fortify_with_census_region(corn)
wheat <- fortify_with_census_region(wheat)

# Plot yield vs. year for the corn dataset
plot_yield_vs_year(corn) +
  # Facet, wrapped by census region
  facet_wrap(vars(census_regions.x))

# Wrap this code into a function
plot_yield_vs_year_by_region <- function(data) {
  plot_yield_vs_year(data) +
    facet_wrap(vars(census_regions.x))
}

# Try it on the wheat dataset
plot_yield_vs_year_by_region(wheat)
```

Radical regional yield analysis! Reassuringly, the corn yields are highest in the West North Central region, the heart of the Corn Belt. For wheat, it looks like the yields are highest in the Wheat Belt (West South Central region) have been overtaken by some other regions.

## Modeling grain yields

![](_images/878.png)

![](_images/879.png)

Running a model
The smooth trend line you saw in the plots of yield over time use a generalized additive model (GAM) to determine where the line should lie. This sort of model is ideal for fitting nonlinear curves. So we can make predictions about future yields, let's explicitly run the model. The syntax for running this GAM takes the following form.

```
gam(response ~ s(explanatory_var1) + explanatory_var2, data = dataset)
```

Here, `s()` means "make the variable smooth", where smooth very roughly means nonlinear.

`mgcv` and `dplyr` are loaded; the `corn` and `wheat` datasets are available.

```{r}
library(mgcv)

# Run a generalized additive model of 
# yield vs. smoothed year and census region
gam(yield_kg_per_ha ~ s(year) + census_regions, data = corn)

# Wrap the model code into a function
run_gam_yield_vs_year_by_region <- function(data) {
  gam(yield_kg_per_ha ~ s(year) + census_regions, data = data)
}

# Try it on the wheat dataset
run_gam_yield_vs_year_by_region(wheat)
```

Masterful modeling! Now that you have a function to run the GAMs, let's make some predictions.

**Making yield predictions**

The fun part of modeling is using the models to make predictions. You can do this using a call to `predict()`, in the following form.

```
predict(model, cases_to_predict, type = "response")
```

`mgcv` and `dplyr` are loaded; GAMs of the corn and wheat datasets are available as `corn_model` and `wheat_model`. A character vector of census regions is stored as `census_regions`.

```{r}
corn_model <- run_gam_yield_vs_year_by_region(corn)
wheat_model <- run_gam_yield_vs_year_by_region(wheat)

# Make predictions in 2050  
predict_this <- data.frame(
  year = 2050,
  census_regions.x = census_regions
) 

# Predict the yield
pred_yield_kg_per_ha <- predict(corn_model, predict_this, type = "response")

predict_this %>%
  # Add the prediction as a column of predict_this 
  mutate(pred_yield_kg_per_ha = pred_yield_kg_per_ha)

# Wrap this prediction code into a function
predict_yields <- function(model, year) {
  predict_this <- data.frame(
    year = year,
    census_regions.x = census_regions
  ) 
  pred_yield_kg_per_ha <- predict(model, predict_this, type = "response")
  predict_this %>%
    mutate(pred_yield_kg_per_ha = pred_yield_kg_per_ha)
}

# Try it on the wheat dataset
predict_yields(wheat_model, 2050)
```

Proficient predicting! The models predict that in 2050, the highest yields will be in the Pacific region for both corn and wheat.

**Do it all over again**

Hopefully, by now, you've realized that the real benefit to writing functions is that you can reuse your code easily. Now you are going to rerun the whole analysis from this chapter on a new crop, barley. Since all the infrastructure is in place, that's less effort than it sounds!

Barley prefers a cooler climate compared to corn and wheat and is commonly grown in the US mountain states of Idaho and Montana.

`dplyr` and `ggplot2`, and `mgcv` are loaded; `fortify_with_metric_units()`, `fortify_with_census_region()`, `plot_yield_vs_year_by_region()`, `run_gam_yield_vs_year_by_region()`, and `predict_yields()` are available.

```{r}
barley <- readRDS("_data/nass.barley.rds")

# Some of the functions we are running
plot_yield_vs_year_by_region <- function(data) {
  plot_yield_vs_year(data) +
    facet_wrap(vars(census_regions))
}

run_gam_yield_vs_year_by_region <- function(data) {
  gam(yield_kg_per_ha ~ s(year) + census_regions, data = data)
}

predict_yields <- function(model, year) {
  predict_this <- data.frame(
    year = year,
    census_regions = census_regions
  ) 
  pred_yield_kg_per_ha <- predict(model, predict_this, type = "response")
  predict_this %>%
    mutate(pred_yield_kg_per_ha = pred_yield_kg_per_ha)
}

fortified_barley <- barley %>% 
  # Fortify with metric units
  fortify_with_metric_units(crop = "barley") %>%
  # Fortify with census regions
  fortify_with_census_region()

# See the result
glimpse(fortified_barley)

plot_yield_vs_year_by_region(fortified_barley)

fortified_barley %>% 
  # Run a GAM of yield vs. year by region
  run_gam_yield_vs_year_by_region()  %>% 
  # Make predictions of yields in 2050
  predict_yields(2050)
```

Brilliant barley analysis! Since all your analysis code was contained in functions, it was really simple to apply it to another dataset. Here you can see that yields are highest in the Mountain region, and the model predicts that this will still be the case in 2050.

## Congratulations

**In Chapter 1 you learned**

- writing your own functions lets you reuse code
- there is a simple process for turning scripts into functions
- data arguments come before detail arguments

**In Chapter 2 you learned**

- defaults can be set using `name = value` syntax
- arguments can be passed between functions using their name or `...`
- checking user inputs can be done using `assertive`

**In Chapter 3 you learned**

- you can return early from a function uding `return()`
- you can prevent return values being printed with `invisible()`
- functions can return multiple values uding lists or attributes.
- R has rules about *scope* that determine which variables can be seen

**In Chapter 4 you learned**

- writing your own functions can be useful for your data analyses
- even simple, one-line functions can be helpful

**More modeling**

Logistic Regression is covered in
- introduction to regression in R
- intermediate regression in R
- generalized linear models in R

Generalized Additive Models are covered in
- nonlinear modeling in R with GAMs

**Tidying models**

Tidying models with broom is covered in
- exploratory data analysis: case study
- machine learning in the Tidyverse
- reshaping data with tidyr

**Unit testing**

Unit testing code is covered in
- developing R packages

**Environments**

Environments are covered in
- object-oriented programming with S3 and R6 in R