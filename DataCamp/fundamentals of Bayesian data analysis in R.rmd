---
title: "Fundamentals of Bayesian Data Analysis in R"
author: "DataCamp - Rasmus Bååth"
date: "2/4/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(repos="https://CRAN.R-project.org")
```

## A first taste of Bayes

![](_images/1775.png)

![](_images/1776.png)

![](_images/1777.png)

## Let's try some Bayesian data analysis

![](_images/1778.png)

![](_images/1779.png)

![](_images/1780.png)

![](_images/1781.png)

![](_images/1782.png)

**Coin flips with prop_model**

The function `prop_model` has been loaded into your workspace. It implements a Bayesian model that assumes that:

- The `data` is a vector of successes and failures represented by `1`s and `0`s.
- There is an unknown underlying proportion of success.
- Prior to being updated with data any underlying proportion of success is equally likely.

Assume you just flipped a coin four times and the result was *heads*, *tails*, *tails*, *heads*. If you code *heads* as a success and *tails* as a failure then the following R codes runs *prop_model* with this data

```r
data <- c(1, 0, 0, 1)
prop_model(data)
```

```{r}
library(tidyverse)
library(ggridges)

prop_model <- function (data = c(), prior_prop = c(1, 1), n_draws = 10000, 
    show_plot = TRUE) 
{
    data <- as.logical(data)
    proportion_success <- c(0, seq(0, 1, length.out = 100), 1)
    data_indices <- round(seq(0, length(data), length.out = min(length(data) + 
        1, 20)))
    post_curves <- map_dfr(data_indices, function(i) {
        value <- ifelse(i == 0, "Prior", ifelse(data[i], "Success", 
            "Failure"))
        label <- paste0("n=", i)
        probability <- dbeta(proportion_success, prior_prop[1] + 
            sum(data[seq_len(i)]), prior_prop[2] + sum(!data[seq_len(i)]))
        probability <- probability/max(probability)
        tibble(value, label, proportion_success, probability)
    })
    post_curves$label <- fct_rev(factor(post_curves$label, levels = paste0("n=", 
        data_indices)))
    post_curves$value <- factor(post_curves$value, levels = c("Prior", 
        "Success", "Failure"))
    p <- ggplot(post_curves, aes(x = proportion_success, y = label, 
        height = probability, fill = value)) + geom_density_ridges(stat = "identity", 
        color = "white", alpha = 0.8, panel_scaling = TRUE, size = 1) + 
        scale_y_discrete("", expand = c(0.01, 0)) + scale_x_continuous("Underlying proportion of success") + 
        scale_fill_manual(values = hcl(120 * 2:0 + 15, 100, 65), 
            name = "", drop = FALSE, labels = c("Prior   ", "Success   ", 
                "Failure   ")) + theme_light(base_size = 18) + 
        theme(legend.position = "top")
    if (show_plot) {
        print(p)
    }
    invisible(rbeta(n_draws, prior_prop[1] + sum(data), prior_prop[2] + 
        sum(!data)))
}
```

```{r}
data <- c(1, 0, 0, 1)
prop_model(data)
```

The model knows it's not close to 0% or close to 100%, but believes it could be anything in between.

**Zombie drugs with prop_model**

If we really were interested in the underlying proportion of heads of this coin then `prop_model` isn't particularly useful. Since it assumes that any underlying proportion of success is equally likely prior to seeing any data it will take a lot of coin flipping to convince `prop_model` that the coin is fair. This model is more appropriate in a situation where we have little background knowledge about the underlying proportion of success.

```{r}
# Update the data and rerun prop_model
data = c(rep.int(0, 11), 1, 1)
prop_model(data)
```

It seems like it's not a perfect drug, but between 5% to 40% cured zombies is better than nothing!

## Samples and posterior summaries

![](_images/1783.png)

![](_images/1784.png)

![](_images/1785.png)

![](_images/1786.png)

![](_images/1787.png)

**Looking at samples from prop_model**

Here again is the `prop_model` function which has been given the data from our zombie experiment where two out of 13 zombies got cured. In addition to producing a plot, `prop_model` also returns a large random sample from the posterior over the underlying proportion of success.

```{r}
data = c(1, 0, 0, 1, 0, 0,
         0, 0, 0, 0, 0, 0, 0)
         
# Extract and explore the posterior
posterior <- prop_model(data)
head(posterior)

# Plot the histogram of the posterior
hist(posterior, breaks = 30, xlim = c(0, 1), col = "palegreen4")
```

Hey, that's a good looking plot!

**Summarizing the zombie drug experiment**

The point of working with samples from a probability distribution is that it makes it easy to calculate new measures of interest. The following tasks are about doing just this!

A *point estimate* is a single number used to summarize what's known about a parameter of interest. It can be seen as a "best guess" of the value of the parameter. A commonly used point estimate is the **median** of the posterior. It's the midpoint of the distribution, and it's equally probable for the parameter value to be larger than the median as it is to be smaller than it.

```{r}
# Calculate the median
median(posterior)
```

So, a best guess is that the drug would cure around 18% of all zombies. Another common summary is to report an interval that includes the parameter of interest with a certain probability. This is called a *credible interval* (CI). With a posterior represented as a vector of samples you can calculate a CI using the `quantile()` function.

```{r}
# Calculate the credible interval
quantile(posterior, c(0.05, 0.95))
```

According to the credible interval, there is a 90% probability that the proportion of zombies the drug would cure is between 6% and 38%. (Here we have to be careful to remember that the percentage of cured zombies and the percentage of probability are two different things.)

Now, there is a rival zombie laboratory that is also working on a drug. They claim that they are certain that their drug cures 7% of the zombies it's administered to. Can we calculate how probable it is that our drug is better? Yes, we can! But it's a two stage process.

```{r}
# Calculate the sum
sum(posterior > 0.07)

# Calculate the probability
sum(posterior > 0.07)/length(posterior)
```

It seems there is a large probability (93%) that our zombie drug is better!

## You've done some Bayesian data analysis!

![](_images/1788.png)

![](_images/1789.png)

## The parts needed for Bayesian inference

![](_images/1790.png)

![](_images/1791.png)

![](_images/1792.png)

**Take a generative model for a spin**

Below you have the R code that implements the generative model we just developed.

```{r}
# The generative zombie drug model

# Set parameters
prop_success <- 0.42
n_zombies <- 100

# Simulating data
data <- c()
for(zombie in 1:n_zombies) {
  data[zombie] <- runif(1, min = 0, max = 1) < prop_success
}

data <- as.numeric(data)
data

# Count cured
data <- sum(data)
data
```

Perfect! Some zombies got cured in this simulation, but far from all.

**Take the binomial distribution for a spin**

It turns out that the generative model you ran last exercise already has a name. It's called the **binomial process** or the **binomial distribution**. In R you can use the `rbinom` function to simulate data from a binomial distribution. The `rbinom` function takes three arguments:

- `n` The number of times you want to run the generative model
- `size` The number of trials. (For example, the number of zombies you're giving the drug.)
- `prob` The underlying proportion of success as a number between `0.0` and `1.0`.

```{r}
# Try out rbinom
rbinom(n = 1, size = 100, prob = 0.42)

# Change the parameters
rbinom(n = 200, size = 100, prob = 0.42)
```

Nice! That's a lot of simulated zombies right there.

## Using a generative model

![](_images/1793.png)

![](_images/1794.png)

![](_images/1795.png)

![](_images/1796.png)

**How many visitors could your site get (1)?**

To get more visitors to your website you are considering paying for an ad to be shown 100 times on a popular social media site. According to the social media site, their ads get clicked on 10% of the time.

```{r}
# Fill in the parameters
n_samples <- 100000
n_ads_shown <- 100
proportion_clicks <- 0.10
n_visitors <- rbinom(n_samples, size = n_ads_shown, 
                     prob = proportion_clicks)

# Visualize n_visitors
hist(n_visitors, breaks = 25, col = "palegreen4")
```

## Representing uncertainty with priors

![](_images/1797.png)

![](_images/1798.png)

**Adding a prior to the model**

You're not so sure that your ad will get clicked on exactly 10% of the time. Instead of assigning `proportion_clicks` a single value you are now going to assign it a large number of values drawn from a probability distribution.

For now, we are going to assume that it's equally likely that `proportion_clicks` could be as low as 0% or as high as 20%. 

```{r}
n_samples <- 100000
n_ads_shown <- 100

# Update proportion_clicks
proportion_clicks <- runif(n = n_samples, min = 0.0, max = 0.2)

n_visitors <- rbinom(n = n_samples, size = n_ads_shown, prob = proportion_clicks)
```

Because the `rbinom` function is vectorized the first value of `proportion_clicks` is used to sample the first value in `n_visitors`, the second value in `proportion_clicks` is used for the second in `n_visitors`, and so on. The result is that the samples in `n_visitors` now also incorporate the uncertainty in what the underlying proportion of clicks could be.

```{r}
# Visualize proportion clicks
hist(proportion_clicks, col = "blue")
```

You shouldn't be surprised to see that the uncertainty over `proportion_clicks` is just as you specified it to be: Uniform between 0.0 and 0.2 (except for some small variations in the height of the bars because we took a random sample using `runif`).

```{r}
# Visualize n_visitors
hist(n_visitors, col = "palegreen4")
```

Uncertainty changed the distribution. We went from a random distribution of about 90% to, with added uncertainty of 0 to 20%, 70% likelihood that someone will click on an ad.

## Bayesian models and conditioning

![](_images/1799.png)

![](_images/1800.png)

![](_images/1801.png)

![](_images/1802.png)

![](_images/1803.png)

![](_images/1804.png)

![](_images/1805.png)

**Update a Bayesian model with data**

You ran your ad campaign, and 13 people clicked and visited your site when the ad was shown a 100 times. You would now like to use this new information to update the Bayesian model.

```{r}
# Create the prior data frame
prior <- data.frame(proportion_clicks, n_visitors)

# Examine the prior data frame
head(prior)
```

The reason we've called it prior is because it represents the uncertainty before (that is, prior to) having included the information in the data. Let's do that now!

`prior$n_visitors` represented the uncertainty over how many visitors you would get because of the ad campaign. But now you know you got exactly 13 visitors.

```{r}
# Create the posterior data frame
posterior <- prior[prior$n_visitors == 13, ]

# Visualize posterior proportion clicks
hist(posterior$proportion_clicks, col = "palegreen4")
```

**How many visitors could your site get (3)?**

In the last exercise, you updated the probability distribution over the underlying proportions of clicks (`proportion_clicks`) using new data. Now we want to use this updated `proportion_clicks` to predict how many visitors we would get if we reran the ad campaign.

The result from the last exercise is still in the data frame `posterior`, but if you look at `posterior$n_visits` you'll see it's just `13` repeated over and over again. This makes sense as `posterior` represents what the model knew about the outcome of the last ad campaign after having seen the data.

```{r}
# Assign posterior to a new variable called prior
prior <- posterior

# Take a look at the first rows in prior
head(prior)

# Replace prior$n_visitors with a new sample and visualize the result
n_samples <-  nrow(prior)
n_ads_shown <- 100
prior$n_visitors <- rbinom(n_samples, size = n_ads_shown,
                           prob = prior$proportion_clicks)
hist(prior$n_visitors, col = "palegreen4")

# Calculate the probability that you will get 5 or more visitors
sum(prior$n_visitors >= 5)/length(prior$n_visitors)
```

It's pretty probable that you'll get more than 5 visitors.

## What have we done?

**Joint Probability Distribution**

![](_images/1806.png)

![](_images/1807.png)

**Chapter 4: Computational Methods**

![](_images/1808.png)

## Four good things with Bayes

![](_images/1809.png)

![](_images/1810.png)

![](_images/1811.png)

![](_images/1812.png)

![](_images/1813.png)

![](_images/1814.png)

![](_images/1815.png)

![](_images/1816.png)

![](_images/1817.png)

![](_images/1818.png)

![](_images/1819.png)

**Explore using the Beta distribution as a prior**

The Beta distribution is a useful probability distribution when you want model uncertainty over a parameter bounded between 0 and 1. Here you'll explore how the two parameters of the Beta distribution determine its shape.

```{r}
# Explore using the rbeta function
beta_sample <- rbeta(n = 1000000, shape1 = 1, shape2 = 1)

# Visualize the results
hist(beta_sample, col = "blue")
```

Right! A Beta(1,1) distribution is the same as a uniform distribution between 0 and 1. It is useful as a so-called *non-informative* prior as it expresses than any value from 0 to 1 is equally likely.

```{r}
# Modify the parameters
beta_sample <- rbeta(n = 1000000, shape1 = -1, shape2 = 1)

# Explore the results
head(beta_sample)
```

Yes, `NaN` stands for *not a number* and the reason you got a lot of `NaN`s is that the Beta distribution is only defined when its shape parameters are positive.

```{r}
# Modify the parameters
beta_sample <- rbeta(n = 1000000, shape1 = 100, shape2 = 100)

# Visualize the results
hist(beta_sample, col = "blue")
```

So the larger the shape parameters are, the more concentrated the beta distribution becomes. When used as a prior, this Beta distribution encodes the information that the parameter is most likely close to 0.5.

```{r}
# Modify the parameters
beta_sample <- rbeta(n = 1000000, shape1 = 100, shape2 = 20)

# Visualize the results
hist(beta_sample, col = "blue")
```

So the larger the `shape1` parameter is the closer the resulting distribution is to 1.0 and the larger the `shape2` the closer it is to 0.

**Change the model to use an informative prior**

The code is the old model you developed from scratch in chapter 2.

```{r}
n_draws <- 100000
n_ads_shown <- 100

# Change the prior on proportion_clicks
proportion_clicks <- 
  rbeta(n_draws, shape1 = 5, shape2 = 95)
n_visitors <- 
  rbinom(n_draws, size = n_ads_shown, 
         prob = proportion_clicks)
prior <- 
  data.frame(proportion_clicks, n_visitors)
posterior <- 
  prior[prior$n_visitors == 13, ]

# This plots the prior and the posterior in the same plot
par(mfcol = c(2, 1))
hist(prior$proportion_clicks, 
     xlim = c(0, 0.25))
hist(posterior$proportion_clicks, 
     xlim = c(0, 0.25))
```

Take a look at the new posterior! Due to the new informative prior it has shifted to the left, favoring lower rates.

## Contrasts and comparisons

![](_images/1820.png)

![](_images/1821.png)

![](_images/1822.png)

![](_images/1823.png)

![](_images/1824.png)

**Fit the model using another dataset**

Let's fit the binomial model to both the video ad data (13 out of 100 clicked) and the new text ad data (6 out of a 100 clicked).

```{r}
# Define parameters
n_draws <- 100000
n_ads_shown <- 100
proportion_clicks <- runif(n_draws, min = 0.0, max = 0.2)
n_visitors <- rbinom(n = n_draws, size = n_ads_shown, 
                     prob = proportion_clicks)
prior <- data.frame(proportion_clicks, n_visitors)

# Create the posteriors for video and text ads
posterior_video <- prior[prior$n_visitors == 13, ]
posterior_text <- prior[prior$n_visitors == 6, ]

# Visualize the posteriors
hist(posterior_video$proportion_clicks, xlim = c(0, 0.25))
hist(posterior_text$proportion_clicks, xlim = c(0, 0.25))
```

Looking at the histogram of `posterior_text` it can be said that the value of the proportion of clicks for the text ad is likely between 0.03 and 0.13.

**Calculating the posterior difference**

The posterior `proportion_clicks` for the video and text ad has been put into a single `posterior` data frame. The reason for `[1:4000]` is because these `proportion_clicks`s are not necessarily of the same length, which they need to be when put into a data frame.

Now it's time to calculate the posterior probability distribution over what the difference in proportion of clicks might be between the video ad and the text ad.

```{r}
posterior <- data.frame(
    video_prop = posterior_video$proportion_clicks[1:4000],
    text_prop  = posterior_text$proportion_click[1:4000])
    
# Calculate the posterior difference: video_prop - text_prop
posterior$prop_diff <- posterior$video_prop - posterior$text_prop

# Visualize prop_diff
hist(posterior$prop_diff, col = "palegreen4")

# Calculate the median of prop_diff
median(posterior$prop_diff)

# Calculate the proportion
mean(posterior$prop_diff > 0)
```

Given the model and the data, the probability that the video ad is better than the text ad is 95%. It's pretty likely that the video ad is better.

## Decision analysis

![](_images/1825.png)

![](_images/1826.png)

![](_images/1827.png)

**A small decision analysis 1**

Each visitor spends `$2.53` on average, a video ad costs `$0.25` and a text ad costs `$0.05`. Let's figure out the probable profit when using video ads and text ads!

```{r}
visitor_spend <- 2.53
video_cost <- 0.25
text_cost <- 0.05

# Add the column posterior$video_profit
posterior$video_profit <- posterior$video_prop * visitor_spend - video_cost

# Add the column posterior$text_profit
posterior$text_profit <- posterior$text_prop * visitor_spend - text_cost

# Visualize the video_profit and text_profit columns
par(mfcol = c(2,1))
hist(posterior$video_profit, xlim = c(-0.2, 0.4))
hist(posterior$text_profit, xlim = c(-0.2, 0.4))
```

Great! Take a look at the two histograms you've plotted, which method seems most profitable, if any?

**A small decision analysis 2**

Using the columns `video_profit` and `text_profit` that you added to `posterior` in the last exercise, let's conclude the decision analysis.

```{r}
# Add the column posterior$profit_diff
posterior$profit_diff <- posterior$video_profit - posterior$text_profit

# Visualize posterior$profit_diff
hist(posterior$profit_diff)

# Calculate a "best guess" for the difference in profits
median(posterior$profit_diff)

# Calculate the probability that text ads are better than video ads
mean(posterior$profit_diff < 0)
```

So it seems that the evidence does not strongly favor neither text nor video ads. But if forced to choose at this point, we would choose text ads.

Even though text ads get a lower proportion of clicks, they are also much cheaper. And, as you have calculated, there is a 63% probability that text ads are better.

## Change anything and everything

![](_images/1828.png)

![](_images/1829.png)

![](_images/1830.png)

![](_images/1831.png)

![](_images/1832.png)

**The Poisson distribution**

The Poisson distribution simulates a process where the outcome is a number of occurrences per day/year/area/unit/etc. Before using it in a Bayesian model, let's explore it!

```{r}
# Simulate from a Poisson distribution and visualize the result
x <- rpois(n = 10000, lambda = 3)
hist(x)
```

Let's say that you run an ice cream stand and on cloudy days you on average sell 11.5 ice creams. It's a cloudy day.

```{r}
x <- rpois(n = 10000, lambda = 11.5)
hist(x)
```

It's still a cloudy day, and unfortunately, you won't break even unless you sell 15 or more ice creams.

```{r}
mean(x >= 15)
```

It is not likely that you will break even on a cloudy day, so we should stay at home.

**Clicks per day instead of clicks per ad**

When you put up a banner on your friend's site you got 19 clicks in a day, how many daily clicks should you expect this banner to generate on average? Now, modify your model, *one piece at a time*, to calculate this.

```{r}
# Change the model according to instructions
n_draws <- 100000
mean_clicks <- runif(n_draws, min = 0, max = 80)
n_visitors <- rpois(n = n_draws, mean_clicks)

prior <- data.frame(mean_clicks, n_visitors)
posterior <- prior[prior$n_visitors == 19, ]

# Visualize mean_clicks
par(mfcol = c(2,1))
hist(prior$mean_clicks)
hist(posterior$mean_clicks)
```

The model is complete! Like before you could now calculate credible probability intervals using quantile or calculate the probability of getting more than, say, 15 clicks next day. But just looking at the posterior probability distribution:

The range of expected daily clicks is 12 to 28 daily clicks on average.

## Bayes is optimal, kind of...

![](_images/1833.png)

## Probability rules

![](_images/1834.png)

![](_images/1835.png)

![](_images/1836.png)

![](_images/1837.png)

**Cards and the sum rule**

A standard French-suited deck of playing cards contains 52 cards; 13 each of hearts (♥), spades (♠), clubs (♣), and diamonds (♦). Assuming that you have a well-shuffled deck in front of you, the probability of drawing any given card is 1/52 ≈ 1.92%.

```{r}
# Calculate the probability of drawing any of the four aces
prob_to_draw_ace <- 1/52 * 4
```

**Cards and the product rule**

Again, assuming that you have a well-shuffled deck in front of you, the probability of drawing any given card is `1/52` ≈ 1.92% . The probability of drawing any of the four aces is `1/52 + 1/52 + 1/52 + 1/52` = `4/52`. Once an ace has been drawn, the probability of picking any of the remaining three is `3/51`. If another ace is drawn the probability of picking any of the remaining two is `2/50`, and so on.

```{r}
# Calculate the probability of picking four aces in a row
prob_to_draw_four_aces <- 4/52 * 3/51 * 2/50 * 1/49
```

Yes! The probability to draw four aces is 4/52 * 3/51 * 2/50 * 1/49 = 0.0004%. Pretty unlikely!

## Calculating likelihoods

![](_images/1838.png)

![](_images/1839.png)

![](_images/1840.png)

![](_images/1841.png)

**From rbinom to dbinom**

Below is currently code that

1. Simulates the number of clicks/visitors (`n_clicks`) from 100 shown ads using the `rbinom` function given that the underlying proportion of clicks is 10%.
2. Calculates the probability of getting 13 visitors (`prob_13_visitors`).

That is, in probability notation it's calculating P(`n_visitors = 13` | `proportion_clicks` = 10%).

```{r}
# Rewrite this code so that it uses dbinom instead of rbinom
n_ads_shown <- 100
proportion_clicks <- 0.1
n_visitors <- rbinom(n = 99999, 
    size = n_ads_shown, prob = proportion_clicks)
prob_13_visitors <- sum(n_visitors == 13) / length(n_visitors)
prob_13_visitors
```

```{r}
n_ads_shown <- 100
proportion_clicks <- 0.1
prob_13_visitors <- dbinom(13, 
    size = n_ads_shown, prob = proportion_clicks)
prob_13_visitors
```

Alright! You got a similar result as with rbinom, but dbinom is much more efficient!

**Calculating probabilities with dbinom**

Below is roughly the code you ended up with in the last exercise.

```{r}
# Change the code to calculate probability distribution of n_visitors
n_ads_shown <- 100
proportion_clicks <- 0.1
n_visitors <- 13
prob <- dbinom(n_visitors, 
    size = n_ads_shown, prob = proportion_clicks)
prob
```

Currently the code calculates P(`n_visitors = 13` | `proportion_clicks` = 10%): The probability of getting 13 visitors *given* that the proportion of clicks is 10%.

Change the code to instead calculate P(`n_visitors` | `proportion_clicks` = 10%): The probability distribution over all possible numbers of visitors.

```{r}
n_ads_shown <- 100
proportion_clicks <- 0.1
n_visitors <- seq(0, 100, 1)
prob <- dbinom(n_visitors, 
    size = n_ads_shown, prob = proportion_clicks)
prob
```

```{r}
# Plot the distribution
plot(n_visitors, prob, type = "h")
```

It seems that with `proportion_clicks` = 10% the most probable `n_visitors` values are between 1 and 20. Now, let's flip the equation:

```{r}
# Change the code according to the instructions
n_ads_shown <- 100
proportion_clicks <- seq(0, 1, by = 0.01)
n_visitors <- 13
prob <- dbinom(n_visitors, 
    size = n_ads_shown, prob = proportion_clicks)
prob

plot(proportion_clicks, prob, type = "h")
```

The plot you just produced almost looks like it shows the probability distribution over different values of `proportion_clicks`, but it does not. For one, the values in `prob` do not sum up to one. What you have calculated is the likelihood of different values of `proportion_clicks` to result in `n_visitors = 13`.

Looking at the plot, the value of `proportion_clicks` that seems to give the maximum likelihood to produce `n_visitors = 13` is `proportion_clicks = 0.13`

What you've found by eyeballing this graph is the so-called *maximum likelihood estimate* of `proportion_clicks`.

## Bayesian calculation













